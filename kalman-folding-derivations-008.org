#+TITLE: Kalman Folding 3: Derivations (WORKING DRAFT)
#+SUBTITLE: Extracting Models from Data, One Observation at a Time
#+AUTHOR: Brian Beckman
#+DATE: <2016-05-03 Tue>
#+EMAIL: bbeckman@34363bc84acc.ant.amazon.com
#+OPTIONS: ':t *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t c:nil
#+OPTIONS: creator:comment d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t
#+OPTIONS: num:t p:nil pri:nil stat:t tags:t tasks:t tex:t timestamp:t toc:t
#+OPTIONS: todo:t |:t
#+SELECT_TAGS: export
#+STARTUP: indent
#+LaTeX_CLASS_OPTIONS: [10pt,oneside,x11names]
#+LaTeX_HEADER: \usepackage{geometry}
#+LaTeX_HEADER: \usepackage{amsmath}
#+LaTeX_HEADER: \usepackage{amssymb}
#+LaTeX_HEADER: \usepackage{amsfonts}
#+LaTeX_HEADER: \usepackage{palatino}
#+LaTeX_HEADER: \usepackage{siunitx}
#+LaTeX_HEADER: \usepackage{esdiff}
#+LaTeX_HEADER: \usepackage{xfrac}
#+LaTeX_HEADER: \usepackage{nicefrac}
#+LaTeX_HEADER: \usepackage{faktor}
#+LaTeX_HEADER: \usepackage[euler-digits,euler-hat-accent]{eulervm}
#+OPTIONS: toc:2

* COMMENT Preliminaries

This section is just about setting up org-mode. It shouldn't export to the
typeset PDF and HTML.

#+BEGIN_SRC emacs-lisp :exports results none
  (defun update-equation-tag ()
    (interactive)
    (save-excursion
      (goto-char (point-min))
      (let ((count 1))
        (while (re-search-forward "\\tag{\\([0-9]+\\)}" nil t)
          (replace-match (format "%d" count) nil nil nil 1)
          (setq count (1+ count))))))
  (update-equation-tag)
  (setq org-confirm-babel-evaluate nil)
  (org-babel-map-src-blocks nil (org-babel-remove-result))
  (slime)
#+END_SRC

#+RESULTS:
: #<buffer *inferior-lisp*>

* Abstract

In /Kalman Folding, Part 1/,[fn:klfl] we present basic, static Kalman filtering
as a functional fold, highlighting the unique advantages of this form for
deploying test-hardened code verbatim in harsh, mission-critical environments.
The examples in that paper are all static, meaning that the states of the model
do not depend on the independent variable, often physical time.

Here, we present mathematical derivations of the basic, static filter. These are
semi-formal sketches that leave many details to the reader, but highlight all
important points that must be rigorously proved. These derivations have several
novel arguments and we strive for much higher clarity and simplicity than is
found in most treatments of the topic.

* Kalman Folding in the Wolfram Language

In this series of papers, we use the Wolfram language[fn:wolf] because it
excels at concise expression of mathematical code. All examples in these papers
can be written in any modern mainstream language that supports higher-order
functions or function pointers. For example, it is easy to write them in C, C++,
Python, any Lisp, not to mention Haskell, Scala, Erlang, and OCaml. 

In /Kalman Folding/,[fn:klfl] we found the following elegant formulation for the
accumulator function of a fold that implements the static Kalman filter:

#+BEGIN_LaTeX
\begin{equation}
\label{eqn:kalman-cume-definition}
\text{kalmanStatic}
\left(
\mathbold{Z}
\right)
\left(
\left\{
\mathbold{x},
\mathbold{P}
\right\},
\left\{
\mathbold{A},
\mathbold{z}
\right\}
\right) =
\left\{
\mathbold{x}+
\mathbold{K}\,
\left(
\mathbold{z}-
\mathbold{A}\,
\mathbold{x}
\right),
\mathbold{P}-
\mathbold{K}\,
\mathbold{D}\,
\mathbold{K}^\intercal
\right\}
\end{equation}
#+END_LaTeX

\noindent where

#+BEGIN_LaTeX
\begin{align}
\label{eqn:kalman-gain-definition}
\mathbold{K}
&=
\mathbold{P}\,
\mathbold{A}^\intercal\,
\mathbold{D}^{-1} \\
\label{eqn:kalman-denominator-definition}
\mathbold{D}
&= \mathbold{Z} +
\mathbold{A}\,
\mathbold{P}\,
\mathbold{A}^\intercal
\end{align}
#+END_LaTeX

\noindent and all quantities are matrices:

- $\mathbold{z}$ is a  ${b}\times{1}$ column vector containing one multidimensional observation
- $\mathbold{x}$ is an ${n}\times{1}$ column vector of /model states/
- $\mathbold{Z}$ is a  ${b}\times{b}$ matrix, the covariance of
  observation noise
- $\mathbold{P}$ is an ${n}\times{n}$ matrix, the theoretical
  covariance of $\mathbold{x}$
- $\mathbold{A}$ is a  ${b}\times{n}$ matrix, the /observation partials/
- $\mathbold{D}$ is a  ${b}\times{b}$ matrix, the Kalman denominator
- $\mathbold{K}$ is an ${n}\times{b}$ matrix, the Kalman gain

In physical or engineering applications, these quantities carry physical
dimensions of units of measure in addition to their matrix dimensions as numbers
of rows and columns. 
If the physical and matrix dimensions of 
$\mathbold{x}$ 
are
$\left[\left[\mathbold{x}\right]\right]
\stackrel{\text{\tiny def}}{=}
(\mathcal{X}, n\times{1})$
and of 
$\mathbold{z}$ 
are
$\left[\left[\mathbold{z}\right]\right]
\stackrel{\text{\tiny def}}{=}
(\mathcal{Z}, b\times{1})$, then

#+BEGIN_LaTeX
\begin{equation}
\label{eqn:dimensional-breakdown}
\begin{array}{lccccr}
\left[\left[\mathbold{Z}\right]\right]                                       &=& (&\mathcal{Z}^2            & b\times{b}&) \\
\left[\left[\mathbold{A}\right]\right]                                       &=& (&\mathcal{Z}/\mathcal{X}  & b\times{n}&) \\
\left[\left[\mathbold{P}\right]\right]                                       &=& (&\mathcal{X}^2            & n\times{n}&) \\
\left[\left[\mathbold{A}\,\mathbold{P}\,\mathbold{A}^\intercal\right]\right] &=& (&\mathcal{Z}^2            & b\times{b}&) \\
\left[\left[\mathbold{D}\right]\right]                                       &=& (&\mathcal{Z}^2            & b\times{b}&) \\
\left[\left[\mathbold{P}\,\mathbold{A}^\intercal\right]\right]               &=& (&\mathcal{X}\,\mathcal{Z} & n\times{b}&) \\
\left[\left[\mathbold{K}\right]\right]                                       &=& (&\mathcal{X}/\mathcal{Z}  & n\times{b}&)
\end{array}
\end{equation}
#+END_LaTeX

Dimensional arguments, regarding both matrix dimensions and physical dimensions,
are invaluable for checking the derivations that follow.

* Derivations

Here, we derive equations \ref{eqn:kalman-cume-definition},
\ref{eqn:kalman-gain-definition} and \ref{eqn:kalman-denominator-definition}.
Again, these derivations are just sketches designed for clarity. 
These derivations only cover the
static Kalman filter, where $\mathbold{x}$ are
fixed, constant, static states of the model. See Bar-Shalom[fn:bars] for
derivations of the time-dependent Kalman filter.

The plan is first to develop expressions for the prior estimate
$\tilde{\mathbold{x}}$ and covariance $\tilde{\mathbold{P}}$, then expressions
for the posterior versions $\hat{\mathbold{x}}$ and $\hat{\mathbold{P}}$,
defining the Kalman gain $\mathbold{K}$ and the denominator matrix
$\mathbold{D}$ along the way. Finally, we derive the convenient expressions for $\mathbold{K}$
and $\mathbold{D}$ that appear in equations \ref{eqn:kalman-cume-definition},
\ref{eqn:kalman-gain-definition}, and \ref{eqn:kalman-denominator-definition}. 

** Notation

The word /vector/ alone means /column vector/ by default. If a quantity is a row
vector, we explicitly say so.
In general, lower-case boldface symbols like $\mathbold{x}$ denote column
vectors.
Row vectors include a superscript /transpose/ symbol, as in
$\mathbold{a}^\intercal$.
We write literal vectors in square brackets, as in
$\left[a, b, \ldots\right]^\intercal$ for a column vector or
$\left[a, b, \ldots\right]$ for a row vector or when we don't care whether
it's a column or row.


Upper-case
boldface symbols like $\mathbold{M}$ denote matrices. Because vectors are
special cases of matrices, some matrices are also vectors. We may use an
upper-case symbol to denote a vector, but we do not use a lower-case symbol to
denote a non-vector matrix.

Juxtaposition, as in
$\mathbold{A}\,\mathbold{x}$ or $\mathbold{A}\,\mathbold{B}$, means matrix multiplication.
We occasionally use a center dot or $\times$ symbol to clarify matrix
multiplication, as in $\mathbold{A}\cdot\mathbold{x}$ or
$\mathbold{A}\times\mathbold{x}$. 


We freely and frequently exploit the following facts without pointing out when
we use them.
- For any matrix $\mathbold{M}$, $\left(\mathbold{M}^\intercal\right)^\intercal = \mathbold{M}$
- For any invertible matrix $\mathbold{M}$, $\left(\mathbold{M}^{-1}\right)^{-1} = \mathbold{M}$
- For any two matrices $\mathbold{A}$ and
  $\mathbold{B}$,
  $\left(\mathbold{A}\,\mathbold{B}\right)^\intercal=\mathbold{B}^\intercal\mathbold{A}^\intercal$
- $\left(\mathbold{A}\,\mathbold{B}\right)^{-1}=\mathbold{B}^{-1}\mathbold{A}^{-1}$
  when the matrices are invertible
- $\mathbold{P}^\intercal$ = $\mathbold{P}$ if and only if $\mathbold{P}$ is
  symmetric

For any matrix $\mathbold{M}$, $\mathbold{M}^2$ means
$\mathbold{M}^\intercal\mathbold{M}$, the transpose of the matrix times the
matrix. Such squared matrices are always square and symmetric.
This notation pertains to vectors, as well, because they are just
special cases of matrices. Thus,
$\mathbold{x}^2=\mathbold{x}^\intercal\mathbold{x}$, the Euclidean
$\mbox{2-\textrm{norm}}$ of $\mathbold{x}$, a scalar; and
$(\mathbold{x}^\intercal)^2 =
(\mathbold{x}^\intercal)^\intercal\cdot
\mathbold{x}^\intercal=
\mathbold{x}\,\mathbold{x}^\intercal$
is the outer product of $\mathbold{x}$ with itself. That outer product is an
$n\times{n}$ square, symmetric matrix, where $n$ is the dimensionality of $\mathbold{x}$. 

When $\mathbold{M}^2$ is invertible, $\mathbold{M}^{-2}$
means the inverse of $\mathbold{M}^2$, namely
$\left(\mathbold{M}^\intercal\mathbold{M}\right)^{-1}$.

We use the term /tall/ to mean a matrix with more rows than columns, that is, an
$m\times{n}$
matrix when
$m>n$. When discussing
$m\times{n}$
matrices, we  usually assume that
$m>n$.
We use the term /wide/ to mean a matrix with
more columns than rows, as in an $n\times{m}$ matrix. We use the term /small/ to
mean $n\times{n}$, and /large/ to mean $m\times{m}$. 

** Definitions

- $t$ :: is the independent variable. In many applications, $t$ represents physical
     time, or an integer index mapped to physical time. It is known and
     non-random. We treat it as a scalar, here, though it is possible to extend
     the theory to a vector $t$.

- $\mathbold{x}$ :: is the (column) vector of $n$ unknown, constant /states/
     of the model. It's a random variable, and we compute estimates and
     covariances /via/ expectation values over its distribution. This symbol
     also means an algebraic variable standing for some particular estimate of
     the states.

- $\mathbold{A}\,\mathbold{x}$ :: the /model/; it predicts an observation at
     time $t$ given an estimate of the states $\mathbold{x}$ and a current
     partials matrix $\mathbold{A}$ that depends on $t$. The model is a
     column vector of dimensionality $b\times{1}$, the dimensionality of an
     observation $\mathbold{z}$.

- $\mathbold{A}$ :: is the /current partials matrix/, the partial derivative of
     the model with respect to the unknown states $\mathbold{x}$, evaluated
     at the current value of the independent variable $t$. We could write
     $\mathbold{A}$ as $\mathbold{A}(t)$, and perhaps we should; it's an
     aesthetic judgment not to write the $t$ dependence explicitly because it
     would make the derivations so much longer and harder to read.  Because the
     model is /linear/, the partials do not depend on $\mathbold{x}$. 
     $\mathbold{A}$ is known, non-random, and depends only on $t$. Generally,
     its dimensionality
     is $b\times{n}$, where $b$ is the dimensionality of an 
     observation $\mathbold{z}$.

- $\tilde{\mathbold{A}}$ :: is the /prior partials matrix/, a matrix that stacks
     all the prior rows of $\mathbold{A}$ that precede the current row. It is
     known, non-random, and $m b\times{n}$, where $m$ is the number of prior
     observations, $b$ is the dimensionality of an 
     observation $\mathbold{z}$, and $n$ is the dimensionality of the states
     $\mathbold{x}$.  Thus
     $\tilde{\mathbold{A}}$ is tall in the typical /overdetermined/ case where
     $m>n$, more observations than states. We do not actually
     realize $\tilde{\mathbold{A}}$ in computer memory because Kalman keeps
     /all information/ in the running covariance matrix. $\tilde{\mathbold{A}}$
     is just a
     useful abstraction in the derivations below.

- $\mathbold{z}$ :: is the /current observation/. It is known and non-random.
     Its dimensionality is $b\times{1}$, $b$ perhaps suggesting `bundle.'

- $\tilde{\mathbold{z}}$ :: is a stack or /batch/ of all prior observations. It
     is known, non-random, $m b\times{1}$. It's a useful abstraction in the
     derivations below. It's not necessary to actually realize it in computer
     memory because we use all its information incrementally by folding.

- ${\tilde{\mathbold{x}}}$ :: the /prior estimate/, the estimate of
     $\mathbold{x}$ given all information we have prior to the current
     observation. It is known, non-random, $n\times{1}$. 

- ${\hat{\mathbold{x}}}$ ::  the /posterior estimate/, the estimate of
     $\mathbold{x}$ given (1) the prior estimate ${\tilde{\mathbold{x}}}$, (2)
     the current partials $\mathbold{A}$, and (3) the current observation
     $\mathbold{z}$. It is known, non-random, $n\times{1}$. It satisfies
     /the Kalman update equation/:

#+BEGIN_LaTeX
\begin{equation}
\label{eqn:kalman-update-equation}
{\hat{\mathbold{x}}} =
{\tilde{\mathbold{x}}} +
\mathbold{K}
\left(
\mathbold{z}-
\mathbold{A}\,
{\tilde{\mathbold{x}}}
\right)
\end{equation}
#+END_LaTeX

\noindent which is equivalent to  the recurrence
$\mathbold{x}\leftarrow\mathbold{x}+\mathbold{K}\,(z-\mathbold{A}\,\mathbold{x})$
used in part 1 of this series.

- ${\tilde{\mathbold{P}}}$ :: /covariance of the priors/, equals
     $\mathbold{Z}\left(
     {\tilde{\mathbold{A}}}^\intercal\,
     {\tilde{\mathbold{A}}}
     \right)^{-1}\stackrel{\text{\tiny def}}{=}
     \mathbold{Z}\,{\tilde{\mathbold{A}}}^{-2}$
     (proof sketch
     below). This is called just $\mathbold{P}$ in part one of this series.
     It is known, non-random, $n\times{n}$. 

- ${\hat{\mathbold{P}}}$ :: /posterior covariance/, satisfies
     ${\hat{\mathbold{P}}}\,
     {\mathbold{A}}^\intercal=
     \mathbold{Z}\,\mathbold{K}=
     \mathbold{Z}\,{\tilde{\mathbold{P}}}\,\mathbold{A}^\intercal\,\mathbold{D}^{-1}$
     (proof sketch below). We calculate it from the prior covariance
     $\tilde{\mathbold{P}}$, the observation-noise covariance $\mathbold{Z}$, and the new
     partials matrix $\mathbold{A}$. 
     It is known, non-random, $n\times{n}$. 

- $\mathbold{A}\,{\tilde{\mathbold{x}}}$ :: the /predicted observation/ given
     the prior estimate ${\tilde{\mathbold{x}}}$ and the current partials matrix
     $\mathbold{A}$. It is a particular evaluation of the model. It is known,
     non-random, $b\times{1}$.

- $\mathbold{z}-\mathbold{A}\,{\tilde{\mathbold{x}}}$ ::  the measurement
     /residual/, the difference between the current observation and the
     predicted observation.

- $\mathbold{\zeta}$ ::  /observation noise/, random, column-vector variable with
     zero mean and unit covariance. Its dimensionality is $b\times{1}$, that of
     the observation $\mathbold{z}$. Its
     mean is
     $E
     \left[
     \mathbold{\zeta}
     \right]=
     \mathbold{0}$ and its covariance is
     $E
     \left[
     \mathbold{\zeta}\,
     \mathbold{\zeta}^\intercal
     \right]=
     \mathbold{Z}$: known, non-random $b\times{b}$.

- $\mathbold{Z}$ :: covariance of the observation noise, $E
     \left[
     \mathbold{\zeta}\,
     \mathbold{\zeta}^\intercal
     \right]=
     \mathbold{Z}$: known, non-random $b\times{b}$.

- $\tilde{\mathbold{z}} = \tilde{\mathbold{A}}\,{\mathbold{x}} + \mathbold{\zeta}$ :: the
     /observation equation/.
     $\tilde{\mathbold{z}}$ is known, non-random, $m b\times{1}$;
     $\tilde{\mathbold{A}}$ is known, non-random, $m b\times{n}$; ${\mathbold{x}}$
     is unknown, random, $n\times{1}$; $\mathbold{\zeta}$ is unknown, random,
     $m b\times{1}$.

- $\mathbold{K}$ :: /Kalman gain/
     $=
     {\tilde{\mathbold{P}}}\,
     \mathbold{A}^\intercal\,
     {\mathbold{D}}^{-1}$ (proof
     sketch below).
     Non-random, $n\times{b}$.

- $\mathbold{D}$ :: /Kalman denominator/
     $=
     \mathbold{Z}+
     \mathbold{A}\,
     {\tilde{\mathbold{P}}}\,
     \mathbold{A}^\intercal$
     (proof sketch below). Non-random, $b\times{b}$f.

** Demonstration that Prior Covariance ${\tilde{\mathbold{P}}} = \mathbold{Z}\,\tilde{\mathbold{A}}^{-2}$

The fact that the prior covariance, $\tilde{\mathbold{P}}$, equals
$\mathbold{Z}\,
\tilde{\mathbold{A}}^{-2}$, which is a tall matrix that stacks all $m$ prior model
partial derivatives, means that all the information about the model is carried
along in one, small $n\times{n}$ matrix. This is the secret to Kalman's
constant-memory usage.

*** Covariance of a Random Vector Variable

The covariance of any random column-vector variable $\mathbold{y}$ is defined as the
expectation value
$E
\left[
\mathbold{y}\,
\mathbold{y}^\intercal
\right]
=
E
\left[
({\mathbold{y}^\intercal})^2
\right]$
\noindent This is the expectation value of an outer product of a column vector
$\mathbold{y}$ and its transpose, $\mathbold{y}^\intercal$. Therefore, it is a
$q\times{q}$ matrix, where $q\times{1}$ is the dimensionality of $\mathbold{y}$.

*** Prior Estimate ${\tilde{\mathbold{x}}}$

One of our random variables is $\mathbold{x}$, the column vector of unknown
states. To calculate its estimate, assume we know the values of all $m$ past
partials ${\tilde{\mathbold{A}}}$ (tall, $m b\times{n}$) and observations
$\tilde{\mathbold{z}}$ (tall, $m b\times{1}$).

Relate $\mathbold{x}$ to the known observations ${\tilde{\mathbold{z}}}$ and the known
partials ${\tilde{\mathbold{A}}}$ through the normally distributed random noise column
vector $\mathbold{\zeta}$ and the observation equation:

#+BEGIN_LaTeX
\begin{equation}
\label{eqn:observation-equation}
{\tilde{\mathbold{z}}}={\tilde{\mathbold{A}}}\,\mathbold{x}+\mathbold{\zeta}
\end{equation}
#+END_LaTeX

*** Sum of Squared Residuals

Consider the
following /performance functional/, computed over the population of
$\mathbold{x}$.

#+BEGIN_LaTeX
\begin{equation*}
J(\mathbold{x})
\stackrel{\text{\tiny def}}{=}
\zeta^2=
\left(
{\tilde{\mathbold{z}}}-
{\tilde{\mathbold{A}}}\,
\mathbold{x}
\right)^2=
\left(
{\tilde{\mathbold{z}}}-
{\tilde{\mathbold{A}}}\,
\mathbold{x}
\right)^\intercal
\cdot
\left(
{\tilde{\mathbold{z}}}-
{\tilde{\mathbold{A}}}\,
\mathbold{x}
\right)
\end{equation*}
#+END_LaTeX

\noindent $J(\mathbold{x})$ is a scalar: the sum of squared residuals. A
/residual/ is a difference between an actual and a predicted observation. To
find the $\mathbold{x}$ that minimizes $J(\mathbold{x})$, we could take the
classic, school approach of setting to zero the partial derivatives of
$J(\mathbold{x})$ with respect to $\mathbold{x}$ and solving the resulting
equations for $\mathbold{x}$. The following is an easier way. Multiply the
residuals across by the wide matrix ${\tilde{\mathbold{A}}}^\intercal$:

#+BEGIN_LaTeX
\begin{equation*}
{\tilde{\mathbold{A}}}^\intercal\,
{\tilde{\mathbold{z}}} - 
{\tilde{\mathbold{A}}}^2\,
\mathbold{x}
\end{equation*}
#+END_LaTeX

\noindent producing an \mbox{$n$-vector}, and then construct a
modified performance functional:

#+BEGIN_LaTeX
\begin{equation*}
J'(\mathbold{x})
\stackrel{\text{\tiny def}}{=}
\left(
{\tilde{\mathbold{A}}}^\intercal\,
{\tilde{\mathbold{z}}} -
{\tilde{\mathbold{A}}}^2\,
\mathbold{x}
\right)^2
=
\left(
{\tilde{\mathbold{A}}}^\intercal\,
{\tilde{\mathbold{z}}} -
{\tilde{\mathbold{A}}}^2\,
\mathbold{x}
\right)^\intercal
\cdot
\left(
{\tilde{\mathbold{A}}}^\intercal\,
{\tilde{\mathbold{z}}} -
{\tilde{\mathbold{A}}}^2\,
\mathbold{x}\right)
\end{equation*}
#+END_LaTeX

\noindent $J(\mathbold{x})$ is minimum with respect to $\mathbold{x}$ if and
only if (iff) $J'(\mathbold{x})$ is minimum. Because $J'(\mathbold{x})$ is
non-negative, when $J'(\mathbold{x})$ /can/ be zero, its minimum /must/ be
zero. $J'(\mathbold{x})$ is zero iff ${\tilde{\mathbold{A}}}^2$, an $n\times{n}$
square matrix, is invertible (non-singular) and

#+BEGIN_LaTeX
\begin{equation*}
\mathbold{x}=
{\tilde{\mathbold{A}}}^{-2}\,
{\tilde{\mathbold{A}}}^\intercal\,
{\tilde{\mathbold{z}}}
\end{equation*}
#+END_LaTeX

\noindent because then

#+BEGIN_LaTeX
\begin{equation*}
{\tilde{\mathbold{A}}}^\intercal\,
{\tilde{\mathbold{z}}}=
{\tilde{\mathbold{A}}}^2\,
\mathbold{x}
\end{equation*}
#+END_LaTeX

We call such a solution for $\mathbold{x}$ the /least-squares estimate/ of
$\mathbold{x}$, the estimate of
$\mathbold{x}$ based on all prior observations.
From now on, we write it as ${\tilde{\mathbold{x}}}$

#+BEGIN_LaTeX
\begin{equation}
\label{eqn:least-squares-estimate}
\tilde{\mathbold{x}}
\stackrel{\text{\tiny def}}{=}
{\tilde{\mathbold{A}}}^{-2}
{\tilde{\mathbold{A}}}^\intercal
{\tilde{\mathbold{z}}} 
\end{equation}
#+END_LaTeX

With this solution, we get a new expression for the performance functional
$J(\mathbold{x})$ that is  useful below. First note that 

#+BEGIN_LaTeX
\begin{alignat}{6}
\notag
{\tilde{\mathbold{A}}}^2\,
{\tilde{\mathbold{A}}}^{-2}
&=
\mathbold{1}
&& \text{}
\\
\notag
{\tilde{\mathbold{A}}}^2\,
{\tilde{\mathbold{A}}}^{-2}
{\tilde{\mathbold{A}}}^\intercal
&=
{\tilde{\mathbold{A}}}^\intercal
&& 
\quad\text{Multiply on right by }\tilde{\mathbold{A}}^\intercal
\\
\notag
{\tilde{\mathbold{A}}}^\intercal\,
{\tilde{\mathbold{A}}}\,
{\tilde{\mathbold{A}}}^{-2}
{\tilde{\mathbold{A}}}^\intercal
&=
{\tilde{\mathbold{A}}}^\intercal
&&
\quad\text{Expand definition of }{\tilde{\mathbold{A}}}^2
\\
\label{eqn:aa2at-is-one}
\therefore
{\tilde{\mathbold{A}}}\,
{\tilde{\mathbold{A}}}^{-2}\,
{\tilde{\mathbold{A}}}^\intercal
&=
\mathbold{1}
&&
\quad\text{Arbitrariness of }\tilde{\mathbold{A}}^\intercal\text{on left}
\end{alignat}
#+END_LaTeX

\noindent Therefore

#+BEGIN_LaTeX
\begin{alignat}{6}
\notag
J(\mathbold{x})
&=
\left(
{\tilde{\mathbold{z}}}-
{\tilde{\mathbold{A}}}\,
\mathbold{x}
\right)^\intercal
\cdot
\left(
{\tilde{\mathbold{z}}}-
{\tilde{\mathbold{A}}}\,
\mathbold{x}
\right)
\\
\notag
&=
\left(
{\tilde{\mathbold{z}}}-
{\tilde{\mathbold{A}}}\,
\mathbold{x}
\right)^\intercal
{\tilde{\mathbold{A}}}\,
{\tilde{\mathbold{A}}}^{-2}\,
{\tilde{\mathbold{A}}}^\intercal
\left(
{\tilde{\mathbold{z}}}-
{\tilde{\mathbold{A}}}\,
\mathbold{x}
\right)
&&
\quad\text{insert }\mathbold{1}\text{ from equation \ref{eqn:aa2at-is-one}}
\\
\notag
&=
\left(
{\tilde{\mathbold{z}}}-
{\tilde{\mathbold{A}}}\,
\mathbold{x}
\right)^\intercal
{\tilde{\mathbold{A}}}\,
{\tilde{\mathbold{A}}}^{-2}\,
{\tilde{\mathbold{A}}}^2\,
{\tilde{\mathbold{A}}}^{-2}\,
{\tilde{\mathbold{A}}}^\intercal
\left(
{\tilde{\mathbold{z}}}-
{\tilde{\mathbold{A}}}\,
\mathbold{x}
\right)
&&
\quad\text{insert }\mathbold{1} = {\tilde{\mathbold{A}}}^2\,{\tilde{\mathbold{A}}}^{-2}
\\
\notag
&=
\left[
\left(
{\tilde{\mathbold{z}}}-
{\tilde{\mathbold{A}}}\,
\mathbold{x}
\right)^\intercal
{\tilde{\mathbold{A}}}\,
{\tilde{\mathbold{A}}}^{-2}
\right]
{\tilde{\mathbold{A}}}^2
\left[
{\tilde{\mathbold{A}}}^{-2}\,
{\tilde{\mathbold{A}}}^\intercal
\left(
{\tilde{\mathbold{z}}}-
{\tilde{\mathbold{A}}}\,
\mathbold{x}
\right)
\right]
&&
\quad\text{Regroup}
\\
\label{eqn:performance-functional-reformed}
&=
(\tilde{\mathbold{x}}-\mathbold{x})^\intercal\,
{\tilde{\mathbold{A}}^2}\,
(\tilde{\mathbold{x}}-\mathbold{x})
&&
\quad\text{Definition of }{\tilde{\mathbold{x}}}
\end{alignat}
#+END_LaTeX

\noindent using the fact that  ${\tilde{\mathbold{A}}^2}$ is symmetric. This has
physical dimensions $\mathcal{Z}^2$ where $\mathcal{Z}$ are the physical
dimensions of the observations $\mathbold{z}$.

*** Prior Covariance $\tilde{\mathbold{P}}$

We now want the covariance of the /residuals/, the differences between
our least-squares estimate $\tilde{\mathbold{x}}$ and the random vector
$\mathbold{x}$:

#+BEGIN_LaTeX
\begin{align}
\label{eqn:covariance-of-x}
\tilde{\mathbold{P}}
\stackrel{\text{\tiny def}}{=}
E
\left[
(\tilde{\mathbold{x}}-x)
(\tilde{\mathbold{x}}-x)^\intercal
\right]
\end{align}
#+END_LaTeX

\noindent  Get $\tilde{\mathbold{x}}-\mathbold{x}$
from the observations and partials at hand as follows:

#+BEGIN_LaTeX
\begin{alignat}{6}
\notag
{\tilde{\mathbold{z}}}
&=
{\tilde{\mathbold{A}}}\,
\mathbold{x} + 
\mathbold{\zeta}
&&
\quad\text{Equation \ref{eqn:observation-equation}}
\\
\notag
{\tilde{\mathbold{A}}}^{-2}\,
{\tilde{\mathbold{A}}}^\intercal\,
{\tilde{\mathbold{z}}}
&=
\mathbold{x} + 
{\tilde{\mathbold{A}}}^{-2}\,
{\tilde{\mathbold{A}}}^\intercal\,
\mathbold{\zeta}
&&
\quad\text{Multiply on left by }{\tilde{\mathbold{A}}}^{-2}\,\tilde{\mathbold{A}}^\intercal
\\
\notag
\tilde{\mathbold{x}}
&=
\mathbold{x} +
{\tilde{\mathbold{A}}}^{-2}\,
{\tilde{\mathbold{A}}}^\intercal\,
\mathbold{\zeta}
&&
\quad\text{Definition of }{\tilde{\mathbold{x}}}
\\
\notag
\therefore
\tilde{\mathbold{x}} -
\mathbold{x} &=
{\tilde{\mathbold{A}}}^{-2}
{\tilde{\mathbold{A}}}^\intercal
\mathbold{\zeta}
\end{alignat}
#+END_LaTeX

\noindent
Now rewrite equation \ref{eqn:covariance-of-x}:

#+BEGIN_LaTeX
\begin{align}
\notag
E
\left[
(\tilde{\mathbold{x}}-x)
(\tilde{\mathbold{x}}-x)^\intercal
\right] &=
E
\left[
{\tilde{\mathbold{A}}}^{-2}
{\tilde{\mathbold{A}}}^\intercal
\mathbold{\zeta}\,
\mathbold{\zeta}^\intercal
({\tilde{\mathbold{A}}}^{-2}
{\tilde{\mathbold{A}}}^\intercal
\mathbold{\zeta})^\intercal
\right] \\
\label{eqn:almost-final-covariance}
&=
{\tilde{\mathbold{A}}}^{-2}
{\tilde{\mathbold{A}}}^\intercal\,
E\left[
\mathbold{\zeta}\,
\mathbold{\zeta}^\intercal
\right]
({\tilde{\mathbold{A}}}^{-2}
{\tilde{\mathbold{A}}}^\intercal)^\intercal
\end{align}
#+END_LaTeX

\noindent  Noise $\mathbold{\zeta}$ is Gaussian, normal, with variance $\mathbold{Z}$.
Equation \ref{eqn:almost-final-covariance} collapses to

#+BEGIN_LaTeX
\begin{align*} 
\tilde{\mathbold{P}} =
{\tilde{\mathbold{A}}}^{-2}
{\tilde{\mathbold{A}}}^\intercal\,
E\left[
\mathbold{\zeta}\,\mathbold{\zeta}^\intercal
\right]
({\tilde{\mathbold{A}}}^{-2}
{\tilde{\mathbold{A}}}^\intercal)^\intercal 
&= 
{\tilde{\mathbold{A}}}^{-2}
{\tilde{\mathbold{A}}}^\intercal\,
\mathbold{Z}\,
({\tilde{\mathbold{A}}}^{-2}
{\tilde{\mathbold{A}}}^\intercal)^\intercal \\
&= 
{\tilde{\mathbold{A}}}^{-2}
{\tilde{\mathbold{A}}}^\intercal\,
\mathbold{Z}\,
{\tilde{\mathbold{A}}}
({\tilde{\mathbold{A}}}^{-2})^\intercal \\
&= 
\mathbold{Z}\,
{\tilde{\mathbold{A}}}^{-2}
{\tilde{\mathbold{A}}}^2
({\tilde{\mathbold{A}}}^{-2})^\intercal \\
&=
\mathbold{Z}\,
({\tilde{\mathbold{A}}}^{-2})^\intercal \\
&=
\mathbold{Z}\,
{\tilde{\mathbold{A}}}^{-2} 
\end{align*}
#+END_LaTeX

\noindent because ${\tilde{\mathbold{A}}}^{-2}$ is symmetric and
because $\mathbold{Z}$
is diagonal and thus commutes with all other matrix products
of compatible matrix dimension. We can now rewrite
the definition of the least squares estimate in equation \ref{eqn:least-squares-estimate}:

#+BEGIN_LaTeX
\begin{equation}
\label{eqn:estimate-of-the-priors}
{\tilde{\mathbold{x}}}=
\mathbold{Z}^{-1}\,
{\tilde{\mathbold{P}}}\,
{\tilde{\mathbold{A}}}^\intercal\,
{\tilde{\mathbold{z}}}
\end{equation}
#+END_LaTeX

** Posterior Estimate $\hat{\mathbold{x}}$ and Covariance $\hat{\mathbold{P}}$

To effect incremental updates of $\mathbold{x}$ and $\mathbold{P}$, we need the
posterior estimate $\hat{\mathbold{x}}$ and covariance $\hat{\mathbold{P}}$ in
terms of the priors $\tilde{\mathbold{x}}$, $\tilde{\mathbold{P}}$, and the new
partials $\mathbold{A}$ and observation $\mathbold{z}$. This is exactly what our
/kalmanStatic/ function from equation \ref{eqn:kalman-cume-definition} does, of course,
in functional form, but we derive the posteriors from scratch to seek
opportunities to define $\mathbold{K}$ and $\mathbold{D}$ and radically shorten
the expressions. 

First, define a new performance functional $J_1(\mathbold{x})$ as the sum of the 
performance of the priors $\tilde{J}(\mathbold{x})$ from equation
\ref{eqn:performance-functional-reformed}, now written with tildes overhead,
and a new term
$J_2(\mathbold{x})$ for the
performance of the new data:

#+BEGIN_LaTeX
\begin{alignat}{6}
J_1(\mathbold{x})
& \stackrel{\text{\tiny def}}{=}
{\tilde{J}}(\mathbold{x}) +
J_2(\mathbold{x})
\\
\notag
{\tilde{J}}(\mathbold{x})
&\stackrel{\text{\tiny def}}{=}
(\tilde{\mathbold{x}}-\mathbold{x})^\intercal\,
{\tilde{\mathbold{A}}^2}\,
(\tilde{\mathbold{x}}-\mathbold{x})
&&
\quad\text{Equation \ref{eqn:performance-functional-reformed}}
\\
\label{eqn:performance-of-new-data}
J_2(\mathbold{x})
&\stackrel{\text{\tiny def}}{=}
\left(
\mathbold{z}-
\mathbold{A}\,
\mathbold{x}
\right)^2
\\
\notag
&=
\left(
\mathbold{z}-
\mathbold{A}\,
\mathbold{x}
\right)^\intercal
\cdot
\left(
\mathbold{z}-
\mathbold{A}\,
\mathbold{x}
\right)
\\
\notag
&=
\mathbold{z}^2 -
2\,
\mathbold{z}\,
\mathbold{A}\,
\mathbold{x} +
\left(
\mathbold{A}\,
\mathbold{x}
\right)^2
\end{alignat}
#+END_LaTeX

This time, I don't have a handy trick for minimizing the performance functional.
Let's find the minimizing $\mathbold{x}$ the classic way: by solving
$d\,J_1(\mathbold{x})/d\,\mathbold{x}=0$. The usual way to write a vector
derivative is with the /nabla/ operator $\nabla$, which produces /gradient/
vectors from scalar functions.

#+BEGIN_LaTeX
\begin{align*}
\nabla{}\,f(\mathbold{x}) &\stackrel{\text{\tiny def}}{=}
\begin{bmatrix}
df(\mathbold{x})/dx_0\\
df(\mathbold{x})/dx_1\\
\vdots\\
df(\mathbold{x})/dx_{n-1}
\end{bmatrix}
\end{align*}
#+END_LaTeX

The particular scalar function we're differentiating is, of course, the new
performance functional
$J_1(\mathbold{x})=
{\tilde{J}}(\mathbold{x})+
J_2(\mathbold{x})$. Because
${\tilde{\mathbold{A}}^2}$ is symmetric,

#+BEGIN_LaTeX
\begin{align*}
\nabla{}\,
{\tilde{J}}(\mathbold{x}) &=
\nabla{}
\left(
(\tilde{\mathbold{x}}-\mathbold{x})^\intercal\,
{\tilde{\mathbold{A}}^2}\,
(\tilde{\mathbold{x}}-\mathbold{x})
\right) \\ &=
-2\,
{\tilde{\mathbold{A}}^2}\,
(\tilde{\mathbold{x}}-\mathbold{x})
\end{align*}
#+END_LaTeX

\noindent and we similarly compute the gradient of
$J_2(\mathbold{x})$, which contains the new observation and partials:

#+BEGIN_LaTeX
\begin{align*}
\nabla\,
J_2(\mathbold{x})
&=
\nabla
\left(
\mathbold{z}^2 -
2\,
\mathbold{z}\,
\mathbold{A}\,
\mathbold{x} +
\left(
\mathbold{A}\,
\mathbold{x}
\right)^2
\right)
\\
&=
2\,
\mathbold{A}^\intercal
\left(
\mathbold{A}\,
\mathbold{x} -
\mathbold{z}
\right)
\\
&=
2\,
\left(
\mathbold{A}^2\,
\mathbold{x}-
\mathbold{A}^\intercal\,
\mathbold{z}
\right)
\end{align*}
#+END_LaTeX

\noindent We can solve the resulting equation on sight, writing the new estimate
with an overhat. We skip many
intermediate steps that become obvious if you reproduce the derivation by hand. Be
aware that $\mathbold{A}^2$ is an outer product, thus a matrix, in the common
case of scalar observations, where $b = 1$ and
$\mathbold{A}$ is a row.

#+BEGIN_LaTeX
\begin{align*}
\nabla{}\,
J_1(\mathbold{x}) 
&= 
\nabla{}\,
{\tilde{J}}
(\mathbold{x}) + 
\nabla{}\,
J_2(\mathbold{x}) 
= 0
\\
&=
{\tilde{\mathbold{A}}}^2\,
\mathbold{x} -
{\tilde{\mathbold{A}}}^2\,
{\tilde{\mathbold{x}}} +
\mathbold{A}^2\,
\mathbold{x} - 
\mathbold{A}^\intercal{}\,
\mathbold{z}
\\
&
\Leftrightarrow
x=\hat{x}
\stackrel{\text{\tiny def}}{=}
\left(
{\tilde{\mathbold{A}}}^2 + 
\mathbold{A}^2
\right)^{-1}
\cdot
\left(
\mathbold{A}^\intercal\,
\mathbold{z} + 
{\tilde{\mathbold{A}}}^2\,
{\tilde{\mathbold{x}}}
\right)
\end{align*}
#+END_LaTeX

Look how pretty this is. Equation \ref{eqn:estimate-of-the-priors} for the
priors gave us the form
$\tilde{\mathbold{x}}= \mathbold{Z}^{-1}\,\tilde{\mathbold{P}}\,
\tilde{\mathbold{A}}^\intercal\, \mathbold{z}$, a covariance times a transform
of the observations by the partials, transposed. The new estimate has exactly
the same form if we regard the first matrix factor $\left(
{\tilde{\mathbold{A}}}^2 + \mathbold{A}^2 \right)^{-1}$ as $\mathbold{Z}^{-1}$ times a covariance and if
we regard /all/ the priors ${\tilde{\mathbold{A}}}\,{\tilde{\mathbold{x}}}$ as a /single/
additional observation to add to the current $\mathbold{z}$. This is really
close to the recurrent  form we want. We get there by some
rewrites. First, define the new covariance as the inverse of the sum of the
old inverse covariance
${\tilde{\mathbold{P}}}^{-1}=
\mathbold{Z}^{-1}\,{\tilde{\mathbold{A}}}^{2}$
and the new inverse covariance
$\mathbold{Z}^{-1}\,{\mathbold{A}}^{2}$:

#+BEGIN_LaTeX
\begin{equation}
\label{eqn:new-p-hat-definition}
{\hat{\mathbold{P}}}
\stackrel{\text{\tiny def}}{=}
\mathbold{Z}\,
\left(
{\tilde{\mathbold{A}}}^2 + \mathbold{A}^2
\right)^{-1}
\end{equation}
#+END_LaTeX

\noindent We can write this as a reciprocal and see that it looks
just like the classic `sum of resistors' formula:

#+BEGIN_LaTeX
\begin{equation*}
\frac{1}{\hat{\mathbold{P}}}
=
\frac{{\tilde{\mathbold{A}}}^2}{\mathbold{Z}} + 
\frac{\mathbold{A}^2}{\mathbold{Z}}
=
\frac{1}{\tilde{\mathbold{P}}} + 
\frac{\mathbold{A}^2}{\mathbold{Z}}
\end{equation*}
#+END_LaTeX

\noindent or

#+BEGIN_LaTeX
\begin{equation*}
\frac{1}{\hat{\mathbold{P}}} -
\frac{\mathbold{A}^2}{\mathbold{Z}}
=
\frac{1}{\tilde{\mathbold{P}}} 
\end{equation*}
#+END_LaTeX


\noindent but, defining

#+BEGIN_LaTeX
\begin{equation}
\label{eqn:kalman-gain-new-definition}
\mathbold{K}
\stackrel{\text{\tiny def}}{=}
\mathbold{Z}^{-1}\,
{\hat{\mathbold{P}}}\,
\mathbold{A}^\intercal
\end{equation}
#+END_LaTeX

\noindent we have

#+BEGIN_LaTeX
\begin{equation*}
\mathbold{Z}\,
\mathbold{K} =
{\hat{\mathbold{P}}}\,
\mathbold{A}^\intercal
\end{equation*}
#+END_LaTeX

\noindent so

#+BEGIN_LaTeX
\begin{align*}
\mathbold{Z}\,
\mathbold{K}\,
\mathbold{A} &=
{\hat{\mathbold{P}}}\,
\mathbold{A}^2
\\
{\hat{\mathbold{P}}}^{-1}\,
\mathbold{K}\,
\mathbold{A}\,
&=
\frac{\mathbold{A}^2}{\mathbold{Z}}
\end{align*}
#+END_LaTeX

\noindent Therefore

#+BEGIN_LaTeX
\begin{align}
\notag
{\hat{\mathbold{P}}}^{-1}\,
(
\mathbold{1}-
\mathbold{K}\,
\mathbold{A}
)
&=
{\tilde{\mathbold{P}}}^{-1} 
\\
\label{eqn:derivation-of-p-is-l-p}
{\hat{\mathbold{P}}} &=
\mathbold{L}\,
{\tilde{\mathbold{P}}}
\end{align}
#+END_LaTeX

\noindent where

#+BEGIN_LaTeX
\begin{equation}
\label{eqn:definition-of-l}
\mathbold{L}\stackrel{\text{\tiny def}}{=}
\mathbold{1}-
\mathbold{K}\,
\mathbold{A}
\end{equation}
#+END_LaTeX

We have
one of our three equivalent expressions for the posterior covariance, which we
can write as a recurrence:

#+BEGIN_LaTeX
\begin{equation}
{{\mathbold{P}}} \leftarrow
\mathbold{L}\,
{{\mathbold{P}}}
\end{equation}
#+END_LaTeX


\noindent Note the following identity for the future:

#+BEGIN_LaTeX
\begin{equation}
\label{eqn:pr2andpa2-is-1}
{\hat{\mathbold{P}}}\,
{\tilde{\mathbold{A}}}^2+
{\hat{\mathbold{P}}}\,
{\mathbold{A}}^2 = \mathbold{Z}
\end{equation}
#+END_LaTeX

\noindent Now rewrite ${\hat{\mathbold{x}}}$,  noting that
equation \ref{eqn:pr2andpa2-is-1} implies that 
$\mathbold{Z}\,
\mathbold{L}
=
\mathbold{Z}\,
(
\mathbold{1}-
\mathbold{K}\,
\mathbold{A}
)=
(
\mathbold{Z}-
{\hat{\mathbold{P}}}\,
\mathbold{A}^2
)=
{\hat{\mathbold{P}}}\,
{\tilde{\mathbold{A}}}^2$.

#+BEGIN_LaTeX
\begin{align*}
\hat{\mathbold{x}}
&=
\left(
{\tilde{\mathbold{A}}}^2 + \mathbold{A}^2
\right)
^{-1}
\cdot
\left(
\mathbold{A}^\intercal\,
z +
{\tilde{\mathbold{A}}}^2\,
{\tilde{\mathbold{x}}}
\right)
\\
&=
\mathbold{Z}^{-1}
\left(
{\hat{\mathbold{P}}}\,
\mathbold{A}^\intercal\,
z +
{\hat{\mathbold{P}}}\,
{\tilde{\mathbold{A}}}^2\,
{\tilde{\mathbold{x}}}
\right)
\\
&=
\mathbold{K}\,
z +
\left(
\mathbold{1} -
\mathbold{K}\,
\mathbold{A}
\right)\,
{\tilde{\mathbold{x}}}
\\
\therefore
\hat{\mathbold{x}}
&=
\tilde{\mathbold{x}}
+
\mathbold{K}\,
\left(
z-
\mathbold{A}\,
\tilde{\mathbold{x}}
\right)
\end{align*}
#+END_LaTeX

We have the update recurrence for the vector estimate $\mathbold{x}$. There remain
two more covariance formulas to derive, namely

#+BEGIN_LaTeX
\begin{equation}
\label{eqn:p-is-lplt-plus-kzkt}
\mathbold{P}\leftarrow
\mathbold{L}\,
\mathbold{P}\,
\mathbold{L}^\intercal +
\mathbold{K}\,
\mathbold{Z}\,
\mathbold{K}^\intercal
\end{equation}
#+END_LaTeX

\noindent and the canonical form,

#+BEGIN_LaTeX
\begin{equation}
\label{eqn:p-is-p-minus-kdkt}
\mathbold{P}\leftarrow
\mathbold{P} -
\mathbold{K}\,
\mathbold{D}\,
\mathbold{K}^\intercal
\end{equation}
#+END_LaTeX

*** Minimizing $J_1({\mathbold{x}})$

The new covariance is defined as

#+BEGIN_LaTeX
\begin{equation}
{\hat{\mathbold{P}}} =
E
\left[
({\hat{\mathbold{x}}}-\mathbold{x})
({\hat{\mathbold{x}}}-\mathbold{x})^\intercal
\right]
\end{equation}
#+END_LaTeX

\noindent Get a new expression for ${\hat{\mathbold{x}}}$:

#+BEGIN_LaTeX
\begin{equation}
{\hat{\mathbold{x}}} =
{\tilde{\mathbold{x}}}+
\mathbold{K}\,
(\mathbold{z}-
\mathbold{A}\,
{\tilde{\mathbold{x}}}) =
\mathbold{K}\,
\mathbold{z} +
\mathbold{L}\,
{\tilde{\mathbold{x}}}
\end{equation}
#+END_LaTeX

\noindent where, again

#+BEGIN_LaTeX
\begin{equation}
\mathbold{L}
=
(\mathbold{1}-
\mathbold{K}\,
\mathbold{A})
=
\mathbold{Z}^{-1}
{\hat{\mathbold{P}}}\,
{\tilde{\mathbold{A}}}^2
\end{equation}
#+END_LaTeX

\noindent
Remembering the observation equation
(\ref{eqn:observation-equation}), write a single instance of it
$\mathbold{z} =
\mathbold{A}\,
\mathbold{x}+
\mathbold{\zeta}$ and find

#+BEGIN_LaTeX
\begin{align}
\notag
{\hat{\mathbold{x}}}
&=
\mathbold{K}\,
\mathbold{A}\,
\mathbold{x} +
\mathbold{K}\,
\mathbold{\zeta} +
\mathbold{L}\,
{\tilde{\mathbold{x}}}
\\
\notag
&=
\left(
\mathbold{1}-
\mathbold{L}
\right)\,
\mathbold{x} +
\mathbold{K}\,
\mathbold{\zeta} +
\mathbold{L}\,
{\tilde{\mathbold{x}}}
\\
&\Rightarrow
\left(
{\hat{\mathbold{x}}}-
\mathbold{x}
\right)=
\mathbold{L}\,
\left(
{\tilde{\mathbold{x}}}-
\mathbold{x}
\right) +
\mathbold{K}\,
\mathbold{\zeta}
\end{align}
#+END_LaTeX

\noindent Remembering that
$E
\left[
\mathbold{\zeta}
\right]=\mathbold{0}$, 
$E
\left[
\mathbold{\zeta}\,
\mathbold{\zeta}^\intercal
\right]=\mathbold{Z}$ and skipping
intermediate steps, find that 

#+BEGIN_LaTeX
\begin{equation}
{\hat{\mathbold{P}}} = 
\mathbold{L}\,
{\tilde{\mathbold{P}}}\,
\mathbold{L}^\intercal + 
\mathbold{K}\,
\mathbold{Z}\,
\mathbold{K}^\intercal
\end{equation}
#+END_LaTeX

\noindent We leave it to the reader to check, with reference to equations
\ref{eqn:dimensional-breakdown}, that the physical dimensions work out. This
completes the derivation of the recurrence equation \ref{eqn:p-is-lplt-plus-kzkt}. 

To get the last form, we need a couple of  small lemmas:

*** Lemma: ${\mathbold{K}}\,{\mathbold{A}}\,{\tilde{\mathbold{P}}}\,{\mathbold{A}^\intercal}={\tilde{\mathbold{P}}}\,{\mathbold{A}^\intercal}\,{\mathbold{A}}\,{\mathbold{K}}$

You can call this ``lemma kapat patak'' if you like.

#+BEGIN_LaTeX
\begin{alignat}{6}
\notag
{\hat{\mathbold{P}}}\,
{\mathbold{A}^\intercal}\,
{\mathbold{A}}\,
{\tilde{\mathbold{P}}}
&= 
{\tilde{\mathbold{P}}}\,
{\mathbold{A}^\intercal}\,
{\mathbold{A}}\,
{\hat{\mathbold{P}}}
&& 
\quad\text{Symmetric matrices commute}
\\
\notag
{\hat{\mathbold{P}}}\,
{\mathbold{A}^\intercal}\,
{\mathbold{A}}\,
{\tilde{\mathbold{P}}}\,
{\mathbold{A}^\intercal}
&= 
{\tilde{\mathbold{P}}}\,
{\mathbold{A}^\intercal}\,
{\mathbold{A}}\,
{\hat{\mathbold{P}}}\,
{\mathbold{A}^\intercal}
&& 
\quad\text{Multiply on right by }\mathbold{A}^\intercal
\\
\label{eqn:kapa-paak-lemma}
\therefore
{\mathbold{K}}\,
{\mathbold{A}}\,
{\tilde{\mathbold{P}}}\,
{\mathbold{A}^\intercal}
&= 
{\tilde{\mathbold{P}}}\,
{\mathbold{A}^\intercal}\,
{\mathbold{A}}\,
{\mathbold{K}}
&& 
\quad\text{Subst def of }\mathbold{K}=
{\hat{\mathbold{P}}}\,
{\mathbold{A}^\intercal}
\end{alignat}
#+END_LaTeX

*** Lemma: ${\mathbold{K}}\,{\mathbold{D}}= {\tilde{\mathbold{P}}}\,{\mathbold{A}}^\intercal$

You can call this ``lemma kay-dee pat'' if you like. It is equivalent to the
main form for $\mathbold{K}$ used in \ref{eqn:kalman-gain-definition}. Assuming

#+BEGIN_LaTeX
\begin{equation}
\label{eqn:kalman-denominator-new-definition}
\mathbold{D}
\stackrel{\text{\tiny def}}{=}
\mathbold{Z} +
\mathbold{A}\,
\tilde{\mathbold{P}}\,
\mathbold{A}^\intercal
\end{equation}
#+END_LaTeX

\noindent we get

#+BEGIN_LaTeX
\begin{alignat}{6}
\notag
{\mathbold{K}}\,\mathbold{Z}
&= 
{\hat{\mathbold{P}}}\,
{\mathbold{A}}^\intercal
&& 
\quad\text{Definition of }\mathbold{K}\text{, equation \ref{eqn:kalman-gain-new-definition}}
\\
\notag
{\mathbold{K}}\,\mathbold{Z}
&= 
\mathbold{Z}\,
\left(
{\tilde{\mathbold{A}}}^2+
{\mathbold{A}^2}
\right)
^{-1}
{\mathbold{A}}^\intercal
&& 
\quad\text{Definition of }{\hat{\mathbold{P}}}\text{, equation \ref{eqn:new-p-hat-definition}}
\\
\notag
\left(
{\tilde{\mathbold{A}}}^2+
{\mathbold{A}^2}
\right)
{\mathbold{K}}
&= 
{\mathbold{A}}^\intercal
&& 
\quad\text{Cancellation of }\mathbold{Z}
\\
\notag
{\tilde{\mathbold{A}}}^2\,
{\mathbold{K}}
+
{\mathbold{A}^2}\,
{\mathbold{K}}
&= 
{\mathbold{A}}^\intercal
&& 
\quad\text{Distributive law}
\\
\notag
{\mathbold{K}}\,\mathbold{Z}
+
{\tilde{\mathbold{P}}}\,
{\mathbold{A}^2}\,
{\mathbold{K}}
&= 
{\tilde{\mathbold{P}}}\,
{\mathbold{A}}^\intercal
&& 
\quad\text{Left multiply by }{\tilde{\mathbold{P}}}\stackrel{\text{\tiny def}}{=}
\mathbold{Z}\,{\tilde{\mathbold{A}}}^{-2}
\\
\notag
{\mathbold{K}}\,\mathbold{Z}
+
{\tilde{\mathbold{P}}}\,
{\mathbold{A}}^\intercal\,
\mathbold{A}\,
{\mathbold{K}}
&= 
{\tilde{\mathbold{P}}}\,
{\mathbold{A}}^\intercal
&& 
\quad\text{expand }{\mathbold{A}^2}
\\
\notag
{\mathbold{K}}\,\mathbold{Z}
+
{\mathbold{K}}\,
{\mathbold{A}}\,
{\tilde{\mathbold{P}}}\,
{\mathbold{A}^\intercal}
&= 
{\tilde{\mathbold{P}}}\,
{\mathbold{A}}^\intercal
&& 
\quad\text{Equation \ref{eqn:kapa-paak-lemma}}
\\
\notag
{\mathbold{K}}
\left(
\mathbold{Z}+
{\mathbold{A}}\,
{\tilde{\mathbold{P}}}\,
{\mathbold{A}^\intercal}
\right)
&= 
{\tilde{\mathbold{P}}}\,
{\mathbold{A}}^\intercal
&& 
\quad\text{}
\\
\label{eqn:kd-pat-lemma}
\therefore
{\mathbold{K}}\,
{\mathbold{D}}
&= 
{\tilde{\mathbold{P}}}\,
{\mathbold{A}}^\intercal
&& 
\quad\text{Definition of }{\mathbold{D}}\text{, equation \ref{eqn:kalman-denominator-new-definition}}
\end{alignat}
#+END_LaTeX

\noindent where we have freely used the fact that the diagonal matrix
$\mathbold{Z}$ commutes with all other matrix products.
This also demonstrates our original definition of the Kalman gain,
$\mathbold{K} =
{\hat{\mathbold{P}}}\,
\mathbold{A}^\intercal\,
\mathbold{D}^{-1}$
from equation \ref{eqn:kalman-gain-definition}.

We now show that 
$\mathbold{K}\,
\mathbold{D}=
{\tilde{\mathbold{P}}}\,
\mathbold{A}^\intercal$
implies
${\hat{\mathbold{P}}} =
{\tilde{\mathbold{P}}} -
\mathbold{K}\,
\mathbold{D}\,
\mathbold{K}^\intercal$.

#+BEGIN_LaTeX
\begin{alignat}{6}
\notag
\mathbold{K}\,
\mathbold{D}\,
&= 
{\tilde{\mathbold{P}}}\,
\mathbold{A}^\intercal 
\\
\notag
\mathbold{K}\,
\left(
\mathbold{Z}+
\mathbold{A}\,
{\tilde{\mathbold{P}}}\,
\mathbold{A}^\intercal
\right)\,
&= 
{\tilde{\mathbold{P}}}\,
\mathbold{A}^\intercal 
&& 
\quad\text{Definition of }{\mathbold{D}}\text{, equation \ref{eqn:kalman-denominator-new-definition}}
\\
\notag
\mathbold{K}\,\mathbold{Z}+
\mathbold{K}\,
\mathbold{A}\,
{\tilde{\mathbold{P}}}\,
\mathbold{A}^\intercal \,
&= 
{\tilde{\mathbold{P}}}\,
\mathbold{A}^\intercal 
&& 
\quad\text{Expand }
\\
\notag
{\hat{\mathbold{P}}}\,
\mathbold{A}^\intercal+
\mathbold{K}\,
\mathbold{A}\,
{\tilde{\mathbold{P}}}\,
\mathbold{A}^\intercal \,
&= 
{\tilde{\mathbold{P}}}\,
\mathbold{A}^\intercal 
&& 
\quad\text{Definition of }\mathbold{K}\text{, equation \ref{eqn:kalman-gain-new-definition}}
\\
\notag
{\hat{\mathbold{P}}}\,
\mathbold{A}^\intercal-
{\tilde{\mathbold{P}}}\,
\mathbold{A}^\intercal
&=
-\mathbold{K}\,
\mathbold{A}\,
{\tilde{\mathbold{P}}}\,
\mathbold{A}^\intercal \,
&& 
\quad\text{Rearrange}
\\
\notag
(
{\hat{\mathbold{P}}}-
{\tilde{\mathbold{P}}}
)\,
\mathbold{A}^\intercal
&=
-\mathbold{K}\,
\mathbold{A}\,
{\tilde{\mathbold{P}}}\,
\mathbold{A}^\intercal \,
&& 
\quad\text{Collect}
\\
\notag
(
{\hat{\mathbold{P}}}-
{\tilde{\mathbold{P}}}
)\,
\mathbold{A}^\intercal
&=
-\mathbold{K}\,
(
\mathbold{K}\,
\mathbold{D}
)^\intercal\,
\mathbold{A}^\intercal \,
&&
\quad\text{Hypothesis and symmetry of }{\tilde{\mathbold{P}}}
\\
\therefore
(
{\hat{\mathbold{P}}}-
{\tilde{\mathbold{P}}}
)\,
\mathbold{A}^\intercal
&=
-(
\mathbold{K}\,
\mathbold{D}\,
\mathbold{K}^\intercal
)\,
\mathbold{A}^\intercal
&&
\quad\text{Symmetry of }\mathbold{D}
\end{alignat}
#+END_LaTeX

\noindent For arbitrary $\mathbold{A}^\intercal$, this can only be true if 
${\hat{\mathbold{P}}} =
{\tilde{\mathbold{P}}} -
\mathbold{K}\,
\mathbold{D}\,
\mathbold{K}^
\intercal$.


* Concluding Remarks

These derivations are helpful for gaining intuition into the underlying
statistics and dimensional structures of the Kalman filter and its many
variants. They are a bit involved, but it is worthwhile to ingest these
fundamentals, especially for those who need to research new filters and
applications. For more rigorous proofs built on a Bayesian perspective, see
Bar-Shalom.[fn:bars]

[fn:affn] https://en.wikipedia.org/wiki/Affine_transformation
[fn:bars] Bar-Shalom, Yaakov, /et al/. Estimation with applications to tracking and navigation. New York: Wiley, 2001.
[fn:bier] http://tinyurl.com/h3jh4kt
[fn:bssl] https://en.wikipedia.org/wiki/Bessel's_correction
[fn:busi] https://en.wikipedia.org/wiki/Business_logic
[fn:cdot] We sometimes use the center dot or the $\times$ symbols to clarify
matrix multiplication. They have no other significance and we can always write
matrix multiplication just by juxtaposing the matrices.
[fn:clos] https://en.wikipedia.org/wiki/Closure_(computer_programming)
[fn:cold] This convention only models so-called /cold observables/, but it's enough to demonstrate Kalman's working over them.
[fn:cons] This is quite similar to the standard --- not  Wolfram's --- definition of a list as a pair of a value and of another list.
[fn:cova] We use the terms /covariance/ for matrices and /variance/ for scalars.
[fn:csoc] https://en.wikipedia.org/wiki/Separation_of_concerns
[fn:ctsc] https://en.wikipedia.org/wiki/Catastrophic_cancellation
[fn:dstr] http://tinyurl.com/ze6qfb3
[fn:elib] Brookner, Eli. Tracking and Kalman Filtering Made Easy, New York: Wiley, 1998. http://tinyurl.com/h8see8k
[fn:fldl] http://tinyurl.com/jmxsevr
[fn:fwik] https://en.wikipedia.org/wiki/Fold_%28higher-order_function%29
[fn:gama] https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem
[fn:intr] http://introtorx.com/
[fn:jplg] JPL Geodynamics Program http://www.jpl.nasa.gov/report/1981.pdf
[fn:just] justified by the fact that $\mathbold{D}$ is a diagonal
matrix that commutes with all other products, therefore its left and right
inverses are equal and can be written as a reciprocal; in fact, $\mathbold{D}$
is a $1\times{1}$ matrix --- effectively a scalar --- in all examples in this paper
[fn:klde] To appear.
[fn:klfl] To appear.
[fn:layi] https://en.wikipedia.org/wiki/Fundamental_theorem_of_software_engineering
[fn:lmbd] Many languages use the keyword /lambda/ for such expressions; Wolfram
uses the name /Function/.
[fn:lmlf] https://en.wikipedia.org/wiki/Lambda_lifting
[fn:lssq] https://en.wikipedia.org/wiki/Least_squares
[fn:ltis] http://tinyurl.com/hhhcgca
[fn:matt] https://www.cs.kent.ac.uk/people/staff/dat/miranda/whyfp90.pdf
[fn:mcmc] https://en.wikipedia.org/wiki/Particle_filter
[fn:musc] http://www1.cs.dartmouth.edu/~doug/music.ps.gz
[fn:ndim] https://en.wikipedia.org/wiki/Nondimensionalization
[fn:patt] http://tinyurl.com/j5jzy69
[fn:pseu] http://tinyurl.com/j8gvlug
[fn:rasp] http://www.wolfram.com/raspberry-pi/
[fn:rcrn] https://en.wikipedia.org/wiki/Recurrence_relation
[fn:rsfr] http://rosettacode.org/wiki/Loops/Foreach
[fn:rxbk] http://www.introtorx.com/content/v1.0.10621.0/07_Aggregation.html
[fn:scan] and of Haskell's scans and folds, and Rx's scans and folds, /etc./
[fn:scla] http://tinyurl.com/hhdot36
[fn:scnd] A state-space form containing a position and derivative is commonplace
in second-order dynamics like Newton's Second Law. We usually employ state-space
form to reduce \(n\)-th-order differential equations to first-order differential
equations by stacking the dependent variable on $n-1$ of its derivatives in the
state vector.
[fn:scnl] http://learnyouahaskell.com/higher-order-functions
[fn:stsp] https://en.wikipedia.org/wiki/State-space_representation
[fn:uncl] The initial uncial (lower-case) letter signifies that /we/ wrote this function; it wasn't supplied by Wolfram.
[fn:wfld] http://reference.wolfram.com/language/ref/FoldList.html?q=FoldList
[fn:wlf1] http://tinyurl.com/nfz9fyo
[fn:wlf2] http://rebcabin.github.io/blog/2013/02/04/welfords-better-formula/
[fn:wolf] http://reference.wolfram.com/language/
[fn:zarc] Zarchan and Musoff, /Fundamentals of Kalman Filtering, A Practical
Approach, Fourth Edition/, Ch. 4


