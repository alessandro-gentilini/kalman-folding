#+TITLE: Kalman Folding 6: Dimensional Analysis (WORKING DRAFT)
#+SUBTITLE: Extracting Models from Data, One Observation at a Time
#+AUTHOR: Brian Beckman
#+DATE: <2016-05-03 Tue>
#+EMAIL: bbeckman@34363bc84acc.ant.amazon.com
#+OPTIONS: ':t *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t c:nil
#+OPTIONS: creator:comment d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t
#+OPTIONS: num:t p:nil pri:nil stat:t tags:t tasks:t tex:t timestamp:t toc:t
#+OPTIONS: todo:t |:t
#+SELECT_TAGS: export
#+STARTUP: indent
#+LaTeX_CLASS_OPTIONS: [10pt,oneside,x11names]
#+LaTeX_HEADER: \usepackage{geometry}
#+LaTeX_HEADER: \usepackage{amsmath}
#+LaTeX_HEADER: \usepackage{amssymb}
#+LaTeX_HEADER: \usepackage{amsfonts}
#+LaTeX_HEADER: \usepackage{palatino}
#+LaTeX_HEADER: \usepackage{siunitx}
#+LaTeX_HEADER: \usepackage{esdiff}
#+LaTeX_HEADER: \usepackage{xfrac}
#+LaTeX_HEADER: \usepackage{nicefrac}
#+LaTeX_HEADER: \usepackage{faktor}
#+LaTeX_HEADER: \usepackage[euler-digits,euler-hat-accent]{eulervm}
#+OPTIONS: toc:2

* COMMENT Preliminaries

This section is just about setting up org-mode. It shouldn't export to the
typeset PDF and HTML.

#+BEGIN_SRC emacs-lisp :exports results none
  (defun update-equation-tag ()
    (interactive)
    (save-excursion
      (goto-char (point-min))
      (let ((count 1))
        (while (re-search-forward "\\tag{\\([0-9]+\\)}" nil t)
          (replace-match (format "%d" count) nil nil nil 1)
          (setq count (1+ count))))))
  (update-equation-tag)
  (setq org-confirm-babel-evaluate nil)
  (org-babel-map-src-blocks nil (org-babel-remove-result))
  (slime)
#+END_SRC

#+RESULTS:
: #<buffer *inferior-lisp*>

* Abstract

In /Kalman Folding, Part 1/,[fn:klfl] we present basic, static Kalman filtering
as a functional fold, highlighting the unique advantages of this form for
deploying test-hardened code verbatim in harsh, mission-critical environments.
In /Kalman Folding 2: Tracking/,[fn:klf2] we reproduce a tracking example from
the literature, showing that these advantages extend to time-dependent, linear
models. Here, we extend that example further to include aerodynamic drag, making the
model nonlinear. We must change the Kalman filter itself to handle such
problems. The resulting class of filters are called Extended Kalman Filters or
EKFs. Other papers in this series feature applications of EKFs to a variety of
problems including navigation and pursuit.

The particular EKF designed here includes integration of non-linear equations of
motion. We integrate these equations by folding over a lazy stream that
generates, on demand, differential updates to the solution. Folds over lazy
streams were introduced in /Kalman Folding 4: Streams and Observables/[fn:klf4]
where we used them to step a static Kalman filter over observations. They also
afford a constant-memory representation for solutions of differential equations,
making them suitable for integration components in constant-memory filters.

* Kalman Folding in the Wolfram Language

In this series of papers, we use the Wolfram language[fn:wolf] because it excels
at concise expression of mathematical code. All examples in these papers can be
directly transcribed to any modern mainstream language that supports closures.
For example, it is easy to write them in C++11 and beyond, Python, any modern
Lisp, not to mention Haskell, Scala, Erlang, and OCaml. Many can be written
without full closures; function pointers will suffice, so they are easy to write
in C. It's also not difficult to add extra arguments to simulate just enough
closure-like support in C to write the rest of the examples in that language.

In /Kalman Folding 2: Tracking/,[fn:klf2] we found the following
formulation for the accumulator function of a fold that implements the linear
dynamic Kalman filter, that is, a filter that can track states that evolve with
time[fn:time] according to a linear transformation.

#+BEGIN_LaTeX
\begin{equation}
\label{eqn:kalman-dynamic-cume-definition}
\begin{matrix}
\textrm{kalmanDynamic}
\left(
\left\{
\mathbold{x},
\mathbold{P}
\right\},
\left\{
\mathbold{Z},
\mathbold{\Xi},
\mathbold{\Phi},
\mathbold{\Gamma},
\mathbold{u},
\mathbold{A},
\mathbold{z}
\right\}
\right) = \\
\begin{Bmatrix}
\mathbold{ x }_{ 2 }+
\mathbold{ K }\,
\left(
\mathbold{ z }-
\mathbold{ A }\,
\mathbold{ x }_{ 2 }
\right), &
\mathbold{ P }_{ 2 }-
\mathbold{ K }\,
\mathbold{ D }\,
\mathbold{ K }^\intercal
\end{Bmatrix}
\end{matrix}
\end{equation}
#+END_LaTeX

\noindent where

#+BEGIN_LaTeX
\begin{align}
\label{eqn:state-propagation-equation}
\mathbold{ x }_{ 2 }
&=
\mathbold{ \Phi  }\,
\mathbold{ x }+
\mathbold{ \Gamma  }\,
\mathbold{ u } \\
\label{eqn:covariance-propagation-equation}
\mathbold{ P }_{ 2 }
&=
\mathbold{ \Xi  }+
\mathbold{ \Phi  }\,
\mathbold{ P }\,
\mathbold{ \Phi  }^{ \intercal  } \\
\label{eqn:kalman-gain-definition}
\mathbold{K}
&=
\mathbold{P}\,
\mathbold{A}^\intercal\,
\mathbold{D}^{-1} \\
\label{eqn:kalman-denominator-definition}
\mathbold{D}
&= \mathbold{Z} +
\mathbold{A}\,
\mathbold{P}\,
\mathbold{A}^\intercal
\end{align}
#+END_LaTeX

\noindent and all quantities are matrices:

- $\mathbold{z}$ is a  ${b}\times{1}$ column vector containing one multidimensional observation
- $\mathbold{x}$ and $\mathbold{x}_{2}$ are ${n}\times{1}$ column vectors of /model states/
- $\mathbold{Z}$ is a  ${b}\times{b}$ matrix, the covariance of
  observation noise
- $\mathbold{P}$ and $\mathbold{P}_2$ are ${n}\times{n}$ matrices, the theoretical
  covariances of $\mathbold{x}$ and $\mathbold{x}_2$, respectively
- $\mathbold{A}$ is a  ${b}\times{n}$ matrix, the /observation partials/
- $\mathbold{D}$ is a  ${b}\times{b}$ matrix, the Kalman denominator
- $\mathbold{K}$ is an ${n}\times{b}$ matrix, the Kalman gain
- $\mathbold{\Xi}$ is a non-dimensionalized $n\times{n}$ integral of /process
  noise/ $\mathbold{\xi}$, namely \newline \(\int_{0}^{\delta t}\mathbold{\Phi}(\tau)\cdot{\left(\begin{array}{c|c}\mathbold{0}&\mathbold{0}\\\hline\mathbold{0}&E\left[\mathbold{\xi}\mathbold{\xi}^{\intercal}\right]\end{array}\right)\cdot\mathbold{\Phi}(\tau)^\intercal\,\textrm{d}\tau}\)
- $\mathbold{\Phi}$ is a non-dimensionalized $n\times{n}$ propagator for $\mathbold{F}$, namely $e^{\mathbold{F}\,{\delta t}}$
- $\mathbold{\Gamma}$ is an $n\times{\dim(\mathbold{u})}$ integral of /system response/, namely \(\int_{0}^{\delta t}{\mathbold{\Phi}(\tau) \cdot \mathbold{G}\,\textrm{d}\tau}\)
- $\mathbold{u}$ is a vector of external /disturbances/ or /control inputs/
- $\delta{t}$ is an increment of time (or, more generally, the independent
  variable of the problem)

\noindent and the time-evolving states satisfy the following differential
equation in /state-space form/:

#+BEGIN_LaTeX
\begin{equation}
\label{eqn:state-space-form}
{\dot{\mathbold{x}}}=
\mathbold{F}\,\mathbold{x}+
\mathbold{G}\,\mathbold{u}+
\mathbold{\xi}
\end{equation}
#+END_LaTeX

\noindent  $\mathbold{F}$, $\mathbold{G}$, and $\mathbold{u}$ may depend
on time, but not on $\mathbold{x}$; that is the meaning of ``linear dynamic'' in
this context. In this paper, we relieve those restrictions
by explicitly integrating non-linear equations of motion and by using
Taylor-series approximations for the gain $\mathbold{K}$ and 
denominator $\mathbold{D}$ matrices. 

** Dimensional Arguments

In physical or engineering applications, these quantities carry physical
dimensions of units of measure in addition to their matrix dimensions as numbers
of rows and columns. Both kinds of dimensions are aspects of the /type/ of a
quantity. Dimensional arguments, like type-arguments more generally, are
invaluable for checking equations.

If the physical and matrix dimensions of 
$\mathbold{x}$ 
are
$\left[\left[\mathbold{x}\right]\right]
\stackrel{\text{\tiny def}}{=}
(\mathcal{X}, n\times{1})$,
of 
$\mathbold{z}$ 
are
$\left[\left[\mathbold{z}\right]\right]
\stackrel{\text{\tiny def}}{=}
(\mathcal{Z}, b\times{1})$, 
of 
$\delta{t}$
are
$\left[\left[\delta{t}\right]\right]
\stackrel{\text{\tiny def}}{=}
(\mathcal{T}, \textrm{scalar})$, 
and of
$\mathbold{u}$
are
$\left[\left[\mathbold{u}\right]\right]
\stackrel{\text{\tiny def}}{=}
(\mathcal{U}, \dim(\mathbold{u})\times{1})$, 
then

#+BEGIN_LaTeX
\begin{equation}
\label{eqn:dimensional-breakdown}
\begin{array}{lccccr}
\left[\left[\mathbold{Z}\right]\right]                                       &=& (&\mathcal{Z}^2            & b\times{b}&) \\
\left[\left[\mathbold{A}\right]\right]                                       &=& (&\mathcal{Z}/\mathcal{X}  & b\times{n}&) \\
\left[\left[\mathbold{P}\right]\right]                                       &=& (&\mathcal{X}^2            & n\times{n}&) \\
\left[\left[\mathbold{A}\,\mathbold{P}\,\mathbold{A}^\intercal\right]\right] &=& (&\mathcal{Z}^2            & b\times{b}&) \\
\left[\left[\mathbold{D}\right]\right]                                       &=& (&\mathcal{Z}^2            & b\times{b}&) \\
\left[\left[\mathbold{P}\,\mathbold{A}^\intercal\right]\right]               &=& (&\mathcal{X}\,\mathcal{Z} & n\times{b}&) \\
\left[\left[\mathbold{K}\right]\right]                                       &=& (&\mathcal{X}/\mathcal{Z}  & n\times{b}&) \\
\left[\left[\mathbold{F}\right]\right]                                       &=& (&\textrm{powers of } 1/\mathcal{T}            & n\times{n}&) \\
\left[\left[\mathbold{\Phi}\right]\right]                                    &=& (&\textrm{dimensionless}   & n\times{n}&) \\
\left[\left[\mathbold{G}\right]\right]                                       &=& (&\mathcal{X}/(\mathcal{U}\mathcal{T}) & n\times{\dim(\mathbold{u})}&) \\
\left[\left[\mathbold{\Gamma}\right]\right]                                  &=& (&\mathcal{X}/\mathcal{U}  & n\times{\dim(\mathbold{u})}&) \\
\left[\left[\mathbold{\Xi}\right]\right]                                     &=& (&\mathcal{X}^2            & n\times{n}&) \\
\end{array}
\end{equation}
#+END_LaTeX

The non-dimensionalization of $\mathbold{F}$ and $\mathbold{\Xi}$ requires
careful analysis and is the subject of another paper in this series. For now,
assume they have been implicitly non-dimensionalized.

\noindent In all examples in this paper, the observations $\mathbold{z}$ are
$1\times{1}$ matrices, equivalent to scalars, so $b=1$, but the theory and code
carry over to multi-dimensional vector observations.

* Tracking with Drag

Let us reproduce Zarchan and Musoff's[fn:zarc] example of tracking a falling
object in one position dimension, with aerodynamic drag. Throughout this series
of papers, we refer to the examples in that reference because they provide solid
benchmarks against which to check our contribution. The difference between our
approach and typical presentations of Kalman-type filters is functional
decomposition: writing code in functional style affords the ability to deploy it
verbatim, even and especially at the binary level, in both laboratory and field.
This ability can make the difference in
a successful application because seemingly insignificant changes, even
instruction order, can make qualitative differences in filter behavior due to
floating-point issues.

To accommodate nonlinearity, we replace equation
\ref{eqn:state-propagation-equation} for time-propagation of the state
$\mathbold{x}$ with explicit numerical integration of the nonlinear equations of
motion. We use an internal fold over a lazy stream of differential updates to
the state, a kind of fold introduced in part 4 of this series.[fn:klf4] This
form runs in constant memory and allows easy change of the integrator, say from
Euler to Runge-Kutta.

** Equations of Motion

To establish a benchmark solution, we solve the differential equations of motion
using Wolfram's built-in numerical integrator. We then introduce our own Euler
and Runge-Kutta integrators and show they produce similar results when folded
over lazy streams. These integrators do not use special features of the Wolfram
language, so they are easy to implement in other languages.

Let $h(t)$ be the height of
the falling object, and let the state vector $\mathbold{x}(t)$ contain $h(t)$
and its first derivative, $\dot{h}(t)$, the speed of descent.

#+BEGIN_LaTeX
\begin{equation*}
\mathbold{x} = 
\begin{bmatrix} { h } (t) \\ \dot { h } (t) \end{bmatrix}
\end{equation*}
#+END_LaTeX

Omitting, for clarity's sake, explicit dependence of $h$ and $\dot{h}$ on time,
the equations of motion are elementary:

#+BEGIN_LaTeX
\begin{equation}
\label{eqn:equations-of-motion}
\begin{bmatrix} \dot { h } \\ \ddot { h }  \end{bmatrix}
=
\begin{bmatrix}
0 & 1 \\
0 & 0
\end{bmatrix}
\begin{bmatrix} h \\ \dot { h }  \end{bmatrix}
+
\begin{bmatrix} 0 \\ -1 - \textrm{sign}({\dot{h}})\,\rho(h)\,{{\dot{h}}^2}/(2\beta)
\end{bmatrix}
\begin{bmatrix} g \end{bmatrix}
\end{equation}
#+END_LaTeX

\noindent where 
- $g$ is the acceleration of Earth's gravitation, about
  $32.2\,\textrm{ft}/{\textrm{s}}^2$
- $\rho(h)$ is the density of air[fn:zerr] in $\textrm{slug}/{\textrm{ft}}^2$; $\rho\,{{\dot{h}}^2}$ has
  units of pressure, that is, $\textrm{slug}/(\textrm{ft}\cdot{\textrm{sec}^2})$
- $\beta = 500\,\textrm{slug}/(\textrm{ft}\cdot{\textrm{sec}^2})$
  is a constant /ballistic coefficient/  of the object in units of pressure (it
  is possible to estimate this coefficient in the filter; here, we
  treat it as a known constant). 

The positive direction is up and we are only concerned with negative velocities
where the object is approaching the ground. We may provisionally replace the
factor $\textrm{sign}({\dot{h}})$ with -1 and keep our eye out for improper
positive speeds. 

In scalar form, the equations are 

#+BEGIN_LaTeX
\begin{equation*}
\ddot { h }
=
g\left(\frac{\rho(h)\,{{\dot{h}}^2}}{2\beta}-1\right)
\end{equation*}
#+END_LaTeX

\noindent or 

#+BEGIN_LaTeX
\begin{equation}
\label{eqn:scalar-equations-of-motion}
\ddot { h }
=
g\left(\frac{A e^{h/k}\,{{\dot{h}}^2}}{2\beta}-1\right)
\end{equation}
#+END_LaTeX

\noindent 
with
$k=22,000\,\left[\textrm{ft}\right]$, the e-folding height of the atmosphere,
and \(A=0.0034\,[\textrm{slug}/{{\textrm{ft}}^3}]\) for the density of
air at $h=0$.

** Built-in Solver

We integrate these equations for thirty seconds
with  the initial conditions $h(0)=200,000\,[ft]$, ${\dot{h}}=-6,000\,[ft/s^2]$
with Wolfram's built-in ~NDSolve~, the numerical
integrator for differential equations, as follows:

#+BEGIN_LaTeX
\begin{verbatim}
With[{g = 32.2, A = 0.0034, k = 22000., beta = 500.},
 With[{x0 = 200000., v0 = -6000., t0 = 0., t1 = 30.},
   NDSolve[{h''[t] == -g + A g (h'[t])^2 Exp[-h[t]/k]/(2. beta),
       h[0] == x0, h'[0] == v0}, h, {t, t0, t1}]]]
\end{verbatim}
#+END_LaTeX

#+CAPTION: Trajectory of a falling object with drag
#+NAME: fig:ndsolve-falling-with-drag-results
[[file:NDSolveFallingWithDrag.png]]

\noindent producing the results in figure [[fig:ndsolve-falling-with-drag-results]].
These results are indistinguishable from those in the reference.

** Stream Solver

We can write the same differential equation as a lazy stream, which uses only
constant memory. Thus, it is suitable for the internals of a Kalman filter. We
implement the integrator as an accumulator function for a ~foldStream~ from
paper 3[fn:klf3] which produces all intermediate results as a new stream:

#+BEGIN_LaTeX
\begin{verbatim}
foldStream[f_, s_, Null[]] := (* acting on an empty stream *)
  {s, Null}; (* produces a singleton stream containing 's' *)
foldStream[f_, s_, {z_, thunk_}] :=
  (* pass in a new thunk that recurses on the old thunk    *)
  {s, foldStream[f, f[s, z], thunk[]] &};
\end{verbatim}
#+END_LaTeX

The simplest integrator is the Euler integrator, which updates a state with its
derivative times a small interval of time: 

#+BEGIN_LaTeX
\begin{verbatim}
eulerAccumulator[{t_, x_}, {dt_, t_, Dx_}] :=
  {t + dt, x + dt Dx[x, t]};
\end{verbatim}
#+END_LaTeX

This is a binary function that takes two compound arguments. The first is an
instance of the accumulation type: a pair of a time ~t~ and a (usually compound)
state ~x~. The second is an element of the input stream, a triple of a time
differential ~dt~, the same time ~t~ that appears in the first argument, and a
function ~Dx~ that computes the derivative of the state given the state and the
time as ~Dx[x,t]~. 

Folding this integrator over the streamed differential equation produces a
streamed solution. The input stream must produce elements of the form
~{dt, t, Dx}~ and, like all streams, contain a thunk that produces the rest of the
stream.[fn:thnk]

#+BEGIN_LaTeX
\begin{verbatim}
dragDStream[Delta : {dt_, t_, Dx_}] :=
  {Delta, dragDStream[{dt, t + dt, Dx}] &};
\end{verbatim}
#+END_LaTeX

This bit contains nothing specific to our example, but just threads around the
integration inputs and increments time. It could be much more rich,
manipulating ~dt~ and ~Dx~ for speed or numerics (/adaptive integration/).

The kernel of our differential equation is the derivative function ~Dx~, which,
for our example, is

#+BEGIN_LaTeX
\begin{verbatim}
With[{g = 32.2, A = 0.0034, k = 22000., beta = 500.},
  dragD[{x_, v_}, t_] := {v, g (A Exp[-x/k] v^2/(2. beta) - 1)}];
\end{verbatim}
#+END_LaTeX

\noindent Integrating the differential equation for thirty seconds looks like this:

#+BEGIN_LaTeX
\begin{verbatim}
(* constants and initial conditions *)
With[{x0 = 200000., v0 = -6000., t0 = 0., t1 = 30., dt = .1},
 takeUntil[
  foldStream[
   eulerAccumulator,
   {t0, {x0, v0}},
   dragDStream[{dt, t0, dragD}]
   ], First[#] > t1 &]] (* predicate on first elements of solution *)
\end{verbatim}
#+END_LaTeX

The type of the result, here, is a lazy stream produced by ~takeUntil~ from the
lazy stream produced by ~foldStream~. Because these streams are lazy, nothing
happens until we demand values for, say, plotting. The results are
indistinguishable from those in figure [[fig:ndsolve-falling-with-drag-results]]. 

The arguments of ~takeUntil~ are a stream and a predicate. The result is a new
stream that pulls values from the original stream, applying the predicate until
it produces ~True~. At that point, the rest of the stream returned by
~takeUntil~ is empty, represented by invocation of the null thunk, ~Null[]~,
The implementation of ~takeUntil~ is in three overloads:

Given an empty stream and any predicate, produce the empty stream:

#+BEGIN_LaTeX
\begin{verbatim}
takeUntil[Null[], _] := Null[];
\end{verbatim}
#+END_LaTeX

Given a stream containing a value ~v~ and a tail ~thunk~, return the empty
stream if the predicate evaluates to ~True~:

#+BEGIN_LaTeX
\begin{verbatim}
takeUntil[{v_, thunk_}, predicate_] /; predicate[v] := Null[];
\end{verbatim}
#+END_LaTeX

Otherwise, recurse by invoking the ~thunk~ in the stream:

#+BEGIN_LaTeX
\begin{verbatim}
takeUntil[{v_, thunk_}, predicate_] :=
  {v, takeUntil[thunk[], predicate] &};
\end{verbatim}
#+END_LaTeX

** What's the Point?

The point of this style of integration is that we can change three aspects of
the integration independently of one another, leaving the others verbatim,
without even recompilation, because we have un-nested and /decomplected/[fn:hick] these aspects:
1. the integrator
2. sophisticated adaptive treatments of the time increment ~dt~ and derivative function ~Dx~
3. the internals of the derivative function ~Dx~

For example, should Euler integration prove inadequate, we can easily substitute
second- or fourth-order Runge-Kutta integrators. The only requirement is that an
integrator must match the integrator's functional interface:

#+BEGIN_LaTeX
\begin{verbatim}
rk2Accumulator[{t_, x_}, {dt_, t_, Dx_}] :=
  With[{dx1 = dt Dx[x, t]},
   With[{dx2 = dt Dx[x + .5 dx1, t + .5 dt]},
    {t + dt, x + (dx1 + dx2)/2.}]];
rk4Accumulator[{t_, x_}, {dt_, t_, Dx_}] :=
  With[{dx1 = dt Dx[x, t]},
   With[{dx2 = dt Dx[x + .5 dx1, t + .5 dt]},
    With[{dx3 = dt Dx[x + .5 dx2, t + .5 dt]},
     With[{dx4 = dt Dx[x + dx3, t + dt]},
      {t + dt, x + (dx1 + 2. dx2 + 2. dx3 + dx4)/6.}]]]];
\end{verbatim}
#+END_LaTeX

Decomplecting these bits also makes them easier to review and verify by hand
because dependencies are lexically localized, making expressions smaller, easier
to memorize and to find on a page.

** Gain and Covariance Updates

 For gains and covariances, we need the best linear approximation of the
 equations of motion so that we have an expression that structurally resembles equation
 \ref{eqn:state-space-form}. When there are no disturbances,
 $\mathbold{G}\,\mathbold{u}=\mathbold{0}$ and the solution of the linear
 equation $\mathbold{\dot{x}}=\mathbold{F}\,\mathbold{x}$ also satisfies
 $\Delta\mathbold{\dot{x}}=\mathbold{F}\,\Delta\mathbold{x}$ for small
 differences $\Delta\mathbold{\dot{x}}$ and $\Delta\mathbold{x}$. We seek a
 similar form for our nonlinear equations of motion because we can linearize
 them around small differences $\Delta{h}$ and $\Delta{\dot{h}}$:

#+BEGIN_LaTeX
\begin{equation}
\begin{bmatrix} \Delta \dot { h } \\ \Delta \ddot { h }
\end{bmatrix}
=
\begin{bmatrix}
\underset {  }{ \frac { \partial \dot { h }  }{ \partial h }  }  &
\underset {  }{ \frac { \partial \dot { h }  }{ \partial \dot { h }  }  }  \\
\frac { \partial \ddot { h }  }{ \partial h }  &
\frac { \partial \ddot { h }  }{ \partial \dot { h }  }
\end{bmatrix}
\begin{bmatrix}
\Delta h \\ \Delta \dot { h }
\end{bmatrix} 
=
\mathbold{F}(\mathbold{x}=[h\,\dot{h}]^\intercal) \cdot
\begin{bmatrix}
\Delta h \\ \Delta \dot { h }
\end{bmatrix} 
\end{equation}
#+END_LaTeX

\noindent 
Thus, with
$k=22,000\,\left[\textrm{ft}\right]$, the e-folding height of the atmosphere,
and \(A=0.0034\,[\textrm{slug}/{{\textrm{ft}}^3}]\) for the density of
air[fn:zerr] at $h=0$,
our linearized system-dynamics matrix is

#+BEGIN_LaTeX
\begin{equation}
\mathbold{F}(\mathbold{x}) =
\begin{bmatrix}
\underset {  }{ 0 }  &
\underset {  }{ 1 }  \\
\frac{-A g {\dot{h}}^2 e^{{h}/{k}}}{2 \beta  k}  &
\frac{A g {\dot{h}} e^{{h}/{k}}}{\beta }
\end{bmatrix}
\end{equation}
#+END_LaTeX

We need $\mathbold{\Phi}=e^{\mathbold{F}t}$ to propagate solutions forward,
because, if
$\mathbold{\dot{x}}=\mathbold{F}\,\mathbold{x}$, then
$e^{\mathbold{F}t}\,\mathbold{x}$(t) effects a Taylor series. To first order, 

#+BEGIN_LaTeX
\begin{align}
\notag
\mathbold{x}(t+\delta{t}) &= e^{\mathbold{F}\,\delta{t}}\,\mathbold{x}(t) \\
\label{eqn:expand-f}      &\approx \left(\mathbold{1} + \mathbold{F}\,\delta{t}\right)\,\mathbold{x}(t) \\
\notag                    &= \mathbold{x}(t) + \mathbold{F}\,\mathbold{x}(t)\,\delta{t} \\
\notag                    &\approx \mathbold{x}(t) + \mathbold{\dot{x}}(t)\,\delta{t}
\end{align}
#+END_LaTeX

\noindent First-order expansions turn out to be enough, so
we take $\mathbold{\Phi}(\delta{t})=\mathbold{1}+\mathbold{F}\,\delta{t}$ for
our propagator matrix. 

We compute the gains and covariances as in equations
\ref{eqn:covariance-propagation-equation}, 
\ref{eqn:kalman-gain-definition}, and
\ref{eqn:kalman-denominator-definition}:

#+BEGIN_LaTeX
\begin{align}
\mathbold{P}
&\leftarrow
\mathbold{\Xi}+
\mathbold{\Phi}\,
\mathbold{P}\,
\mathbold{\Phi}^\intercal
\end{align}
#+END_LaTeX

\noindent where $\Xi$, integral of the process noise, is 

#+BEGIN_LaTeX
\begin{equation}
\left(\sigma_\xi\right)^2\cdot
\begin{bmatrix}
 \underset{}{\frac{{\delta t}^3}{3}}
&
 \underset{}{{{{\mathbold{F}_{22}}} {\delta t}^3}/{3}+\frac{{\delta t}^2}{2}}
\\
 {{{\mathbold{F}_{22}}} {\delta t}^3}/{3}+\frac{{\delta t}^2}{2} 
&
 {{{\mathbold{F}_{22}}}^2 {\delta t}^3}/{3}+{{\mathbold{F}_{22}}} {\delta t}^2+{\delta t}
\end{bmatrix}
\end{equation}
#+END_LaTeX

\noindent with matrix element $\mathbold{F}_{22}$ evaluated at the current state
$\mathbold{x}$.

** COMMENT Time-Dependent States

Suppose the states $\mathbold{x}$ suffer time evolution by a linear
transformation $\mathbold{F}$ and an additional /disturbance/ or /control/ input
$\mathbold{u}$, linearly transformed by $\mathbold{G}$.
These new quantities may
be functions of time, but not of $\mathbold{x}$ lest the equations be
non-linear. Write
the time derivative of $\mathbold{x}$ as

#+BEGIN_LaTeX
\begin{equation*}
{\dot{\mathbold{x}}}(t)=\mathbold{F}\,\mathbold{x}(t)+\mathbold{G}\,\mathbold{u}(t)
\end{equation*}
#+END_LaTeX

We often leave off the explicit denotation of time-dependence for improved readability:

#+BEGIN_LaTeX
\begin{equation*}
{\dot{\mathbold{x}}}=\mathbold{F}\,\mathbold{x}+\mathbold{G}\,\mathbold{u}
\end{equation*}
#+END_LaTeX

Generalize by adding /random process/ noise $\mathbold{\xi}$ to the state
derivative:

#+BEGIN_LaTeX
\begin{equation}
\label{eqn:state-space-form}
{\dot{\mathbold{x}}}=
\mathbold{F}\,\mathbold{x}+
\mathbold{G}\,\mathbold{u}+
\mathbold{\xi}
\end{equation}
#+END_LaTeX

This is standard /state-space form/[fn:stsp] for
differential equations. Solving these equations is beyond the scope of
this paper, but suffice it to say that we need certain time integrals of
$\mathbold{F}$, $\mathbold{G}$, and $\mathbold{\xi}$ as inputs to the filter.
These are

#+BEGIN_LaTeX
\begin{equation}
\label{eqn:definition-of-Phi}
\mathbold{\Phi}(\delta t)\stackrel{\text{\tiny def}}{=}
e^{\mathbold{F}\,{\delta t}}=
\mathbold{1}+
\frac{\mathbold{F}^2{\delta t^2}}{2!}+
\frac{\mathbold{F}^3{\delta t^3}}{3!}+
\cdots
\end{equation}
#+END_LaTeX

#+BEGIN_LaTeX
\begin{equation}
\label{eqn:definition-of-Gamma}
\mathbold{\Gamma}(\delta t)\stackrel{\text{\tiny def}}{=}
\int_{0}^{\delta t}{\mathbold{\Phi}(\tau) \cdot \mathbold{G}\,\textrm{d}\tau } 
\end{equation}
#+END_LaTeX

#+BEGIN_LaTeX
\begin{equation}
\label{eqn:definition-of-Xi}
\mathbold{\Xi}(\delta t)\stackrel{\text{\tiny def}}{=}
\int_{0}^{\delta t}\mathbold{\Phi}(\tau)\cdot{
\begin{pmatrix}
      0 & \cdots  &       0 \\
\vdots  & \ddots  & \vdots  \\ 
      0 & \cdots  & E\left[\mathbold{ \xi  }\mathbold{ \xi  }^{ \intercal  }\right] 
\end{pmatrix}\cdot\mathbold{\Phi}(\tau)^\intercal\,\textrm{d}\tau}
\end{equation}
#+END_LaTeX

\noindent where $\delta t$ is an increment of time used to advance the filter
discretely. Again, we frequently omit denoting of explicit dependence
on $\delta t$ for improved readability.

** COMMENT Recurrences for Dynamics

The transitions of a state (and its covariance) from time $t$ to the next state
(and covariance) at time
$t+\delta t$ follow these recurrences:

#+BEGIN_LaTeX
\begin{align}
\label{eqn:transition-of-state}
\mathbold{x}
&\leftarrow
\mathbold{\Phi}\,
\mathbold{x}+
\mathbold{\Gamma}\,
\mathbold{u} \\
\mathbold{P}
&\leftarrow
\mathbold{\Xi}+
\mathbold{\Phi}\,
\mathbold{P}\,
\mathbold{\Phi}^\intercal
\end{align}
#+END_LaTeX

These equations appear plausible on inspection and you can verify that they
satisfy equation \ref{eqn:state-space-form}.

** COMMENT The Foldable Filter

These tiny changes are all that is needed to add state evolution to the Kalman
filter:

#+BEGIN_LaTeX
\begin{verbatim}
kalman[Zeta_][{x_, P_}, {Xi_, Phi_, Gamma_, u_, A_, z_}] :=
 Module[{x2, P2, D, K},
  x2 = Phi.x + Gamma.u;
  P2 = Xi + Phi.P.Transpose[Phi];
  (* after this, it's identical to the static filter *)
  D = Zeta + A.P2.Transpose[A];
  K = P2.Transpose[A].inv[D];
  {x2 + K.(z - A.x2), P2 - K.D.Transpose[K]}]\end{verbatim}
#+END_LaTeX

*** Test

Check that it reproduces the test case above for the static filter:

#+BEGIN_LaTeX
\begin{verbatim}
With[{ (* make some constant matrices *)
   Xi = zero[4], Zeta = id[1],
   Phi = id[4], Gamma = zero[4, 1], u = zero[1]},
  Fold[
   kalman[Zeta],
   {col[{0, 0, 0, 0}], id[4]*1000.0},
   Map[ Join[{Xi, Phi, Gamma, u}, #]&, 
    {{{{1,  0., 0.,  0.}}, { -2.28442}}, 
     {{{1,  1., 1.,  1.}}, { -4.83168}}, 
     {{{1, -1., 1., -1.}}, {-10.46010}}, 
     {{{1, -2., 4., -8.}}, {  1.40488}}, 
     {{{1,  2., 4.,  8.}}, {-40.8079}}}]]]
\end{verbatim}
#+END_LaTeX

** COMMENT Dynamics of a Falling Object

Let $h(t)$ be the height of
the falling object, and let the state vector $\mathbold{x}(t)$ contain $h(t)$
and its first derivative, $\dot{h}(t)$, the speed of descent.[fn:scnd]

#+BEGIN_LaTeX
\begin{equation*}
\mathbold{x} = 
\begin{bmatrix} { h } (t) \\ \dot { h } (t) \end{bmatrix}
\end{equation*}
#+END_LaTeX

\noindent The system dynamics are elementary:

#+BEGIN_LaTeX
\begin{equation*}
\begin{bmatrix} \dot { h } (t) \\ \ddot { h } (t) \end{bmatrix}
=
\begin{bmatrix}
0 & 1 \\
0 & 0
\end{bmatrix}
\begin{bmatrix} h(t) \\ \dot { h } (t) \end{bmatrix}
+
\begin{bmatrix} 0 \\ 1 \end{bmatrix}
\begin{bmatrix} g \end{bmatrix}
\end{equation*}
#+END_LaTeX

\noindent where $g$ is the acceleration of Earth's gravitation, about
$-32.2\textrm{ft}/{\textrm{s}}^2$ (note the minus sign). We read out the
dynamics matrices:

#+BEGIN_LaTeX
\begin{equation*}
\begin{matrix}
\mathbold{F} = \begin{bmatrix}0 & 1 \\0 & 0\end{bmatrix}, &
\mathbold{G} = \begin{bmatrix} 0 \\ 1 \end{bmatrix}, &
\mathbold{u} = \begin{bmatrix} g \end{bmatrix}
\end{matrix}
\end{equation*}
#+END_LaTeX

\noindent and their integrals from equations \ref{eqn:definition-of-Phi},
\ref{eqn:definition-of-Gamma}, and \ref{eqn:definition-of-Xi}

#+BEGIN_LaTeX
\begin{equation*}
\begin{matrix}
\mathbold{\Phi} =
\begin{bmatrix}
1  & \delta t  \\
0  & 1 
\end{bmatrix}, &
\mathbold{\Gamma} = 
\begin{bmatrix}
{{\delta t}^2}/{2}  \\
\delta t
\end{bmatrix}, &
\mathbold{\Xi} =
E\left[\mathbold{ \xi  }\mathbold{ \xi  }^{ \intercal  }\right]
\begin{bmatrix}
\sfrac { { \delta t }^{ 3 } }{ 3 }  & \sfrac { { \delta t }^{ 2 } }{ 2 }  \\
\sfrac { { \delta t }^{ 2 } }{ 2 }  & \delta t
\end{bmatrix}
\end{matrix}
\end{equation*}
#+END_LaTeX

#+CAPTION: Simulated tracking of a falling object
#+NAME: fig:big-results
[[file:BigResults.png]]

\noindent We test this filter over a sequence of fake
observations tracking an object from an initial height of $400,000\,\textrm{ft}$
and initial speed of $-6,000\,\textrm{ft}/\textrm{s}$ and from time $t=\si{0}{s}$
to $t=\si{57.5}{s}$, just before impact at $h=0\textrm{ft}$. We take one
observation every tenth of a second, so $\delta t={0.10}\,\textrm{s}$. We compare the
two states $h(t)$ and $\dot{h}(t)$ with ground truth and their residuals with
the theoretical sum of squared residuals in the matrix $\mathbold{P}$. The
results are shown in figure [[fig:big-results]], showing good statistics over five
consecutive runs and qualitatively matching the results in the reference.

The ground truth is

#+BEGIN_LaTeX
\begin{equation*}
h(t) = h_0 + {\dot{h}}_0\,t + g\,t^2/2
\end{equation*}
#+END_LaTeX

\noindent where

#+BEGIN_LaTeX
\begin{equation*}
\begin{matrix}
h_0 = 400,000\,\textrm{ft}, & {\dot{h}}_0 = -6,000\,\textrm{ft}/\textrm{sec}
\end{matrix}
\end{equation*}
#+END_LaTeX

\noindent and we generate fake noisy observations by sampling a Gaussian
distribution of zero mean and standard deviation $1,000\,\textrm{ft}$. We do not
need process noise for this example. It's often added during debugging and of a
Kalman filter to compensate for underfitting or overfitting an inappropriate
model. It's also appropriate when we know that the process is stochastic or
noisy and have an estimate of its covariance.

* Concluding Remarks

It's easy to add system dynamics to a static Kalman filter. Expressed as the
accumulator function for a fold, the filter is decoupled from the environment in
which it runs. We can run exactly the same code, even and especially the same
binary, over arrays in memory, lazy streams, asynchronous observables, any data
source that can support a /fold/ operator. Such flexibility of deployment allows
us to address the difficult issues of modeling, statistics, and numerics in
friendly environments where we have large memories and powerful debugging tools,
then to deploy with confidence in unfriendly, real-world environments where we
have small memories, asynchronous, real-time data delivery, and seldom more than
logging for forensics.

[fn:affn] https://en.wikipedia.org/wiki/Affine_transformation
[fn:bars] Bar-Shalom, Yaakov, /et al/. Estimation with applications to tracking and navigation. New York: Wiley, 2001.
[fn:bier] http://tinyurl.com/h3jh4kt
[fn:bssl] https://en.wikipedia.org/wiki/Bessel's_correction
[fn:busi] https://en.wikipedia.org/wiki/Business_logic
[fn:cdot] We sometimes use the center dot or the $\times$ symbols to clarify
matrix multiplication. They have no other significance and we can always write
matrix multiplication just by juxtaposing the matrices.
[fn:clos] https://en.wikipedia.org/wiki/Closure_(computer_programming)
[fn:cold] This convention only models so-called /cold observables/, but it's enough to demonstrate Kalman's working over them.
[fn:cons] This is quite similar to the standard --- not  Wolfram's --- definition of a list as a pair of a value and of another list.
[fn:cova] We use the terms /covariance/ for matrices and /variance/ for scalars.
[fn:csoc] https://en.wikipedia.org/wiki/Separation_of_concerns
[fn:ctsc] https://en.wikipedia.org/wiki/Catastrophic_cancellation
[fn:dstr] http://tinyurl.com/ze6qfb3
[fn:elib] Brookner, Eli. Tracking and Kalman Filtering Made Easy, New York: Wiley, 1998. http://tinyurl.com/h8see8k
[fn:fldl] http://tinyurl.com/jmxsevr
[fn:fwik] https://en.wikipedia.org/wiki/Fold_%28higher-order_function%29
[fn:gama] https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem
[fn:hick] ``Decomplecting'' is a term coined by Rich Hickey for un-braiding and
un-nesting bits of software.
[fn:intr] http://introtorx.com/
[fn:jplg] JPL Geodynamics Program http://www.jpl.nasa.gov/report/1981.pdf
[fn:just] justified by the fact that $\mathbold{D}$ is a diagonal
matrix that commutes with all other products, therefore its left and right
inverses are equal and can be written as a reciprocal; in fact, $\mathbold{D}$
is a $1\times{1}$ matrix --- effectively a scalar --- in all examples in this paper
[fn:klde] B. Beckman, /Kalman Folding 3: Derivations/, to appear.
[fn:klf2] B. Beckman, /Kalman Folding 2: Tracking/, to appear.
[fn:klf3] B. Beckman, /Kalman Folding 3: Derivations/, to appear.
[fn:klf4] B. Beckman, /Kalman Folding 3: Streams and Observable/, to appear.
[fn:klfl] B. Beckman, /Kalman Folding, Part 1/, to appear.
[fn:layi] https://en.wikipedia.org/wiki/Fundamental_theorem_of_software_engineering
[fn:lmbd] Many languages use the keyword /lambda/ for such expressions; Wolfram
uses the name /Function/.
[fn:lmlf] https://en.wikipedia.org/wiki/Lambda_lifting
[fn:lols] Let Over Lambda
[fn:lssq] https://en.wikipedia.org/wiki/Least_squares
[fn:ltis] http://tinyurl.com/hhhcgca
[fn:matt] https://www.cs.kent.ac.uk/people/staff/dat/miranda/whyfp90.pdf
[fn:mcmc] https://en.wikipedia.org/wiki/Particle_filter
[fn:musc] http://www1.cs.dartmouth.edu/~doug/music.ps.gz
[fn:ndim] https://en.wikipedia.org/wiki/Nondimensionalization
[fn:patt] http://tinyurl.com/j5jzy69
[fn:pseu] http://tinyurl.com/j8gvlug
[fn:rasp] http://www.wolfram.com/raspberry-pi/
[fn:rcrn] https://en.wikipedia.org/wiki/Recurrence_relation
[fn:rsfr] http://rosettacode.org/wiki/Loops/Foreach
[fn:rxbk] http://www.introtorx.com/content/v1.0.10621.0/07_Aggregation.html
[fn:scan] and of Haskell's scans and folds, and Rx's scans and folds, /etc./
[fn:scla] http://tinyurl.com/hhdot36
[fn:scnd] A state-space form containing a position and derivative is commonplace
in second-order dynamics like Newton's Second Law. We usually employ state-space
form to reduce \(n\)-th-order differential equations to first-order differential
equations by stacking the dependent variable on $n-1$ of its derivatives in the
state vector.
[fn:scnl] http://learnyouahaskell.com/higher-order-functions
[fn:stsp] https://en.wikipedia.org/wiki/State-space_representation
[fn:thnk] Wolfram's ampersand postfix operator can covert its operand into a thunk.
[fn:time] In most applications, the independent variable is physical time,
however, it need not be. For convenience, we use the term /time/ to mean /the independent variable of the problem/ simply because it is shorter to write. 
[fn:uncl] The initial uncial (lower-case) letter signifies that /we/ wrote this function; it wasn't supplied by Wolfram.
[fn:wfld] http://reference.wolfram.com/language/ref/FoldList.html?q=FoldList
[fn:wlf1] http://tinyurl.com/nfz9fyo
[fn:wlf2] http://rebcabin.github.io/blog/2013/02/04/welfords-better-formula/
[fn:wolf] http://reference.wolfram.com/language/
[fn:zarc] Zarchan and Musoff, /Fundamentals of Kalman Filtering, A Practical
Approach, Fourth Edition/, Ch. 4
[fn:zerr] Zarchan and Musoff, on page 228, report $0.0034$ for the density of air in
$\textrm{slug}/\textrm{ft}^3$ at the surface; we believe the correct
value is about $0.00242$ but continue with $0.0034$ for comparison's sake.

